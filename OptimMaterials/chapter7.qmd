# The MM algorithm with applications to regularized regression

Chapter 7 introduces the MM (Majorize-Minimize in minimization problems, and Minorize-Maximize for maximization problems). The essence of the MM algorithm is to use a surrogate function $g(x|a)$ to minimize a complicating function $f(x)$. The majorizing function $g(x|a)$ has the properties that it touches $f(x)$ at the supporting point: $f(a) = g(a|a)$, and that it lies above $f(x)$, such that $g(x|a) \geq f(x)$. 

__Example: A majorizing algorithm for the median__

```{r}
MM_median <- function(x, start, maxit, tol) {
  theta <- numeric(length = maxit)
  theta[1] <- start
  t <- 1
  conv <- FALSE
  
  while (!conv) {
    t <- t+1
    theta[t] <- 1/(sum(1/abs(x-theta[t-1]))) * sum(x / abs(x - theta[t-1]))
    
    if (t == maxit | abs(theta[t] - theta[t-1]) < tol) {
      conv <- TRUE
    }
  }
  theta[1:t]
}

x <- c(1,0,9,5,1)
MM_median(x, mean(x), 50, 1e-10)

median(x)
```

We obtained the same solution.

__Example: Multiple linear lasso regression__

```{r}
#| cache: true
set.seed(1)

N <- 10000
P <- 100
B <- c(1:10/5, rep(0, 90))

S <- matrix(0.5, P, P)
diag(S) <- 1
X <- matrix(rnorm(N*P), N, P) %*% chol(S)
Y <- X %*% B + rnorm(N, 0, 10)

X <- scale(X)
Y <- scale(Y)

cv_lasso <- glmnet::cv.glmnet(X, Y, alpha = 1, standardize = F,
                              intercept = FALSE)
lasso <- glmnet::glmnet(X, Y, alpha = 1, standardize = F, 
                        intercept = F,
                        lambda = cv_lasso$lambda.min)


MM_lasso <- function(X, Y, start, lambda, threshold, maxit, tol) {

  b <- matrix(0, maxit, ncol(X))
  b[1, ] <- start
  
  t <- 1
  conv <- FALSE
  
  while(!conv) {
    t <- t+1
    D <- diag(1/(abs(b[t-1,]) + threshold))
    b[t, ] <- solve(t(X) %*% X + lambda/2 * D) %*% t(X) %*% Y
    if (t == maxit | sum(abs(b[t] - b[t-1])) < tol) conv <- TRUE
  }
  round(b[1:t,],5)
}

own_lasso <- MM_lasso(X = X, 
                      Y = Y, 
                      start = runif(rep(1, P)), 
                      lambda = 2*N*cv_lasso$lambda.min, 
                      threshold = 1e-7,
                      maxit = 500, 
                      tol = 1e-14)

dplyr::bind_cols(glmnet = lasso$beta[1:15,],
                 MM_lasso = own_lasso[nrow(own_lasso), 1:15, drop=F] |> c()) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

The results are not the same, but they do come quite close.

```{r}
#| cache: true
n <- 100; p <- 1000
s <- matrix(0.95, p, p)
diag(s) <- 1

x <- matrix(rnorm(n*p), n, p) %*% chol(s)
b <- runif(p)
y <- x %*% b + rnorm(n, 0, 500)

out <- MM_lasso(x, y, runif(p), 10000, .000001, 500, 1e-15)
out[nrow(out), ][abs(out[nrow(out), ]) > 0.00001]
```

# Exercises

__1. Write a script to generate data with the following structure: $p = 9$ predictors with the following covariance structure__
$$
\boldsymbol{R} = \begin{pmatrix}
1 & 0.8 & 0.7 & 0 & 0 & 0 & 0 & 0 & 0 \\
0.8 & 1 & 0.6 & 0 & 0 & 0 & 0 & 0 & 0 \\
0.7 & 0.6 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
0   & 0   & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
0   & 0   & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
0   & 0   & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
0   & 0   & 0 & 0 & 0 & 0 & 1 & 0.2 & 0.4 \\
0   & 0   & 0 & 0 & 0 & 0 & 0.2 & 1 & 0.3 \\
0   & 0   & 0 & 0 & 0 & 0 & 0.4 & 0.3 & 1 
\end{pmatrix}
$$
__and $y_i = \boldsymbol{x}_i'\boldsymbol{\beta} + e_i$ with $\boldsymbol{\beta} = [0,1,2,0,0,3,1.5,1.5,0]'$ and $e_i \sim \mathcal{N}(0,10)$.

_Solution_

```{r}
set.seed(123)
N <- 1000
P <- 9
R <- diag(P)
R[1:3, 1:3] <- c(1, 0.8, 0.7, 0.8, 1, 0.6, 0.7, 0.6, 1)
R[7:9, 7:9] <- c(1, 0.2, 0.4, 0.2, 1, 0.4, 0.4, 0.3, 1)
# X <- matrix(rnorm(N*P), N, P) %*% chol(R)
B <- c(0, 1, 2, 0, 0, 3, 1.5, 1.5, 0)
# Y <- X %*% B + rnorm(N, 0, 10)

Xrand <- matrix(rnorm(N*P), N, P)
Xc <- Xrand - rep(1, N) %*% t(colMeans(Xrand))
S <- svd(Xc)
newX <- sqrt(N-1) * S$u %*% chol(R)
Y <- newX %*% B + 10*rnorm(N)
```

__2. Implement the MM algorithm for the elastic net.__

```{r}
library(ggplot2)

MM_elastic_net <- function(X, Y, start, alpha, eps, l1, l2, maxit, tol) {
  b <- matrix(0, maxit, ncol(X))
  b[1, ] <- start
  t <- 1
  conv <- FALSE
  
  purrr::map2(l1, l2, function(L1, L2) {
    while(!conv) {
      t <- t + 1
      D <- solve(diag(c(abs(b[t-1, ]) + eps)))
      b[t, ] <- solve(t(X) %*% X + alpha * L1/2 * D + (1-alpha) * L2 * diag(ncol(X))) %*% t(X) %*% Y
      b[t, which(abs(b[t, ]) < 1e-5)] <- 0
      
      if (sum(abs(b[t, ] - b[t-1])) < tol | maxit == t) {
        conv <- T
      }
    }
    b[t, ]
  })
}

L <- expand.grid(L1 = c(20, 15, 11, 10, 8, 6, 5, 4, 3, 2, 1, 0.9, 0.8, 0.7, 0.65, 
                        0.6, 0.5, 0.4, 0.35, 0.25, 0.2, 0.15, 0.125, 0.10, 0.09, 
                        0.08, 0.05, 0.02, 0.015, 0.001, 0.0001, 0.00001, 0) * 2*N,
                 L2 = c(0, 0.0001, 0.5, 1) * 2*N)
out <- MM_elastic_net(newX, Y, runif(P), alpha = 0.5, eps = 0.001, 
                      l1 = L$L1, l2 = L$L2, maxit = 1000, tol = 1e-14)

results <- tibble::tibble(
  L1 = L$L1,
  L2 = L$L2,
  B = out
) |>
  tidyr::unnest(B) |>
  dplyr::mutate(Beta = rep(paste0(1:P), length(out)))

results |>
  dplyr::mutate(L1 = factor(L1 / N / 2),
                L2 = factor(L2 / N / 2)) |>
  ggplot(aes(x = L1, y = B, col = Beta, group = Beta)) +
  geom_line() +
  facet_wrap(~L2) +
  theme_minimal() +
  scale_color_viridis_d(name = expression(beta)) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6)) +
  labs(y = expression(beta))
```

