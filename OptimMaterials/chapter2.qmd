# Motivating Problems

Chapter 2 on motivating problems is the first chapter that actually entails exercises.

## Exercises (2.7 in the notes)

**1. Consider the multinomial likelihood in @eq-multin for a model (for a two-way contingency table) assuming independence. Can you simplify the likelihood?** 
$$
\sum_{j=1}^R \sum_{k=1}^C n_{jk} \ln(\pi_{jk})~~~~~~~~~~~~~~~~ \sum_{j=1}^R \sum_{k=1}^C \pi_{jk}=1
$$ {#eq-multin}

*Solution*

$$
\begin{aligned}
\ell(\pi) &= \sum_{j=1}^R \sum_{k=1}^C n_{jk} \ln(\pi_{jk}) \\
&= \sum_{j=1}^R \sum_{k=1}^C n_{jk} \ln(\pi_{j+} \cdot \pi_{+k}) \\
&= \sum_{j=1}^R \sum_{k=1}^C n_{jk} \ln \pi_{j+} + n_{jk} \ln \pi_{+k} \\
&= \sum_{j=1}^R n_{j+} \ln \pi_{j+} + \sum_{k=1}^C n_{+k} \ln \pi_{+k}
\end{aligned}
$$ {#eq-Q1}

**2. In a mixed model, optimization is carried out using the marginal likelihood (the likelihood with the random effects integrated out). Define the marginal likelihood for the one-way random effects ANOVA model.**

One-way random effects ANOVA with group-specific effects $\mu_j \sim \mathcal{N}(0, \sigma^2_{\mu})$, and 
$$
y_{ij} = \beta + \mu_j + \epsilon_{ij},
$$ 
with $\epsilon \sim \mathcal{N}(0, \sigma^2_\epsilon)$, with $a$ groups indexed $j$, and $n_j$ individuals in every group.

*Solution*

So, the likelihood consists of two components. For the individuals within each group, we have

$$
\prod^{n_j}_{i=1} \frac{1}{\sqrt{2\pi\sigma^2_{\epsilon}}} 
\exp \Bigg(
-\frac{(y_{ij} - \beta - \mu_j)^2}{2\sigma^2_{\epsilon}}
\Bigg),
$$

whereas for the groups themselves, we have

$$
\prod^{a}_{j=1} \frac{1}{\sqrt{2\pi\sigma^2_{\mu}}} 
\exp \Bigg(
-\frac{\mu_j^2}{2\sigma^2_{\mu}}
\Bigg).
$$

Combining these components, and integrating out the random effects, we obtain the marginal likelihood 
$$
\prod^{a}_{j=1}
\int
\prod^{n_j}_{i=1} 
\frac{1}{\sqrt{2\pi\sigma^2_{\epsilon}}}
\exp \Bigg(
-\frac{(y_{ij} - \beta - \mu_j)^2}{2\sigma^2_{\epsilon}}
\Bigg)
\frac{1}{\sqrt{2\pi\sigma^2_{\mu}}} 
\exp \Bigg(
-\frac{\mu_j^2}{2\sigma^2_{\mu}}
\Bigg)
d\mu_j.
$$

__3. Suppose you do a simple linear regression analysis using a $t_\nu$-distribution for the residuals (density: $f_\nu(y) = C \sqrt{\lambda} \Big(1 + \frac{\lambda(y-\mu)^2}{\nu}\Big)^{-\frac{\nu+1}{2}}$ where $\mu$ is the mean (for $\nu > 1$), $\lambda$ is a scale parameter and $C$ is a normalizing constraint that does not depend on $\mu$ or $\lambda$). Define the (log-)likelihood for $n$ observations $(y_i, x_i)$, such that $\mu_i = \beta_0 + \beta_1x_i$.__

*Solution*

$$
\begin{aligned}
L(\beta) &= \prod_{i=1}^n C\sqrt{\lambda} \Bigg(1 + \frac{\lambda (y_i-\beta_0-\beta_1x_i)^2}{\nu}  \Bigg)^{-\frac{\nu+1}{2}}, \\
\ell(\beta) &= N \ln C +  \frac{N}{2} \ln \lambda - \sum^n_{i=1} \frac{\nu + 1}{2} \ln \Bigg(1 + \frac{\lambda(y_i - \beta_0 - \beta_1x_i)^2}{\nu}\Bigg)
\end{aligned}
$$
