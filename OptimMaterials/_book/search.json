[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization and Numerical Methods Solutions",
    "section": "",
    "text": "This project has two purposes. First, it is an attempt to organize my solutions to the course Optimization and Numerical Methods in a structured way. Second, it provides a justification to try and learn Quarto."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Chapter 1",
    "section": "",
    "text": "No exercises."
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Motivating Problems",
    "section": "",
    "text": "Chapter 2 on motivating problems is the first chapter that actually entails exercises."
  },
  {
    "objectID": "chapter2.html#exercises-2.7-in-the-notes",
    "href": "chapter2.html#exercises-2.7-in-the-notes",
    "title": "2  Motivating Problems",
    "section": "2.1 Exercises (2.7 in the notes)",
    "text": "2.1 Exercises (2.7 in the notes)\n1. Consider the multinomial likelihood in Equation 2.1 for a model (for a two-way contingency table) assuming independence. Can you simplify the likelihood?\n\\[\n\\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk})~~~~~~~~~~~~~~~~ \\sum_{j=1}^R \\sum_{k=1}^C \\pi_{jk}=1\n\\tag{2.1}\\]\nSolution\n\\[\n\\begin{aligned}\n\\ell(\\pi) &= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{j+} \\cdot \\pi_{+k}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln \\pi_{j+} + n_{jk} \\ln \\pi_{+k} \\\\\n&= \\sum_{j=1}^R n_{j+} \\ln \\pi_{j+} + \\sum_{k=1}^C n_{+k} \\ln \\pi_{+k}\n\\end{aligned}\n\\tag{2.2}\\]\n2. In a mixed model, optimization is carried out using the marginal likelihood (the likelihood with the random effects integrated out). Define the marginal likelihood for the one-way random effects ANOVA model.\nOne-way random effects ANOVA with group-specific effects \\(\\mu_j \\sim \\mathcal{N}(0, \\sigma^2_{\\mu})\\), and\n\\[\ny_{ij} = \\beta + \\mu_j + \\epsilon_{ij},\n\\]\nwith \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2_\\epsilon)\\), with \\(a\\) groups indexed \\(j\\), and \\(n_j\\) individuals in every group.\nSolution\nSo, the likelihood consists of two components. For the individuals within each group, we have\n\\[\n\\prod^{n_j}_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg),\n\\]\nwhereas for the groups themselves, we have\n\\[\n\\prod^{a}_{j=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg).\n\\]\nCombining these components, and integrating out the random effects, we obtain the marginal likelihood\n\\[\n\\prod^{a}_{j=1}\n\\int\n\\prod^{n_j}_{i=1}\n\\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg)\n\\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg)\nd\\mu_j.\n\\]\n3. Suppose you do a simple linear regression analysis using a \\(t_\\nu\\)-distribution for the residuals (density: \\(f_\\nu(y) = C \\sqrt{\\lambda} \\Big(1 + \\frac{\\lambda(y-\\mu)^2}{\\nu}\\Big)^{-\\frac{\\nu+1}{2}}\\) where \\(\\mu\\) is the mean (for \\(\\nu > 1\\)), \\(\\lambda\\) is a scale parameter and \\(C\\) is a normalizing constraint that does not depend on \\(\\mu\\) or \\(\\lambda\\)). Define the (log-)likelihood for \\(n\\) observations \\((y_i, x_i)\\), such that \\(\\mu_i = \\beta_0 + \\beta_1x_i\\).\nSolution\n\\[\n\\begin{aligned}\nL(\\beta) &= \\prod_{i=1}^n C\\sqrt{\\lambda} \\Bigg(1 + \\frac{\\lambda (y_i-\\beta_0-\\beta_1x_i)^2}{\\nu}  \\Bigg)^{-\\frac{\\nu+1}{2}}, \\\\\n\\ell(\\beta) &= N \\ln C +  \\frac{N}{2} \\ln \\lambda - \\sum^n_{i=1} \\frac{\\nu + 1}{2} \\ln \\Bigg(1 + \\frac{\\lambda(y_i - \\beta_0 - \\beta_1x_i)^2}{\\nu}\\Bigg)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Basic tools",
    "section": "",
    "text": "Chapter 3 introduces basic tools for optimization problems, such as Taylor Series Expansion, and introduces the exponential family."
  },
  {
    "objectID": "chapter3.html#exercises-3.7-in-the-book",
    "href": "chapter3.html#exercises-3.7-in-the-book",
    "title": "3  Basic tools",
    "section": "3.1 Exercises (3.7 in the book)",
    "text": "3.1 Exercises (3.7 in the book)\n1. Consider \\(f(x) = \\frac{e^x}{1 + e^x}\\). Derive the third-order Taylor series expansion of this function at \\(x = 0\\), and make a graph with the function and the third-order Taylor series expansion at \\(x = 0\\).\nSolution\n\\[\n\\begin{aligned}\nf(x) &= \\frac{e^x}{1+e^x} \\\\\nf'(x) &= \\frac{e^x(1+e^x)}{(1+e^x)^2} - \\frac{e^{2x}}{(1+e^x)^2} = \\frac{e^x}{(1+e^x)^2} \\\\\nf''(x) &= \\frac{e^x(1+e^x)^2 - e^x 2(1 + e^x)e^x}{(1 + e^x)^4} \\\\\n&= \\frac{e^x(1+e^x)^2 - 2e^{2x}}{(1+e^x)^3} \\\\\n&= \\frac{e^x - e^{ 2x}}{(1+e^x)^3} \\\\\nf'''(x) &= \\frac{(e^x-2e^{2x})(1+e^x)^3 - (e^x - e^{2x}) 3 (1+e^x)^2e^x}{(1+e^x)^6} \\\\\n&= \\frac{e^x - 2e^{2x} + e^{2x} - 2e^{3x} - 3e^{2x} + 3e^{3x}}{(1+e^x)^4} \\\\\n&= \\frac{e^x - 4e^{2x} + e^{3x}}{(1+e^x)^4},\n\\end{aligned}\n\\]\nusing Taylor’s theorem, we get\n\\[\n\\begin{aligned}\nf(x) & \\approx \\sum^n_{k=0} \\frac{f^{(k)}(x_0)}{k!} (x - x_0)^k \\\\\n&= \\frac{1}{2} + \\frac{1}{4}x + 0 - \\frac{1}{48}x^3.\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\nfx <- function(x) exp(x) / (1 + exp(x))\nfx1 <- function(x) exp(x) / (1 + exp(x))^2\nfx2 <- function(x) (exp(x) - exp(2*x)) / (1 + exp(x))^3\nfx3 <- function(x) (exp(x) - 4*exp(2*x) + exp(3*x)) / (1 + exp(x))^4\n\ntaylor <- function(x, root) {\n  fx(root) + fx1(root) * (x - root) + fx2(root) / 2 * (x - root)^2 + fx3(root) / 6 * (x - root)^3\n}\n\nggplot() +\n  geom_function(fun = fx) +\n  geom_function(fun = taylor, args = list(root = 0), col = \"orange\") +\n  xlim(-3, 3) +\n  theme_minimal() +\n  labs(x = \"X\", y = expression(italic(f(X))),\n       title = \"Third-order Taylor Series Expansion\")\n\n\n\n\n2. Consider the function: \\(f(x) = e^{x_1}(4x_1^2 + 2x_2^2 + 4x_1x_2 + 2x_2 + 1)\\). Make a contour plot of this function (let both axes run from -2 to 2) at function values \\(0.2, 0.4, 0.7, 1, 1.7, 1.75, 1.8, 2, 3, 4, 5, 6, 20\\). Derive the second-order Taylor series at \\(\\boldsymbol{x} = (0.5, -1)'\\) and \\(\\boldsymbol{x} = (-0.75, 1)'\\).\nSolution\nContour plot\n\nfx12 <- function(x1, x2) {\n  exp(x1) * (4*x1^2 + 2*x2^2 + 4*x1*x2 + 2*x2 + 1)\n}\n\nexpand.grid(x1 = -200:200/100,\n            x2 = -200:200/100) |>\n  dplyr::mutate(z = fx12(x1, x2)) |>\n  ggplot(aes(x = x1, y = x2, z = z)) +\n  stat_contour_filled(breaks = c(0, 0.2, 0.4, 0.7, 1, 1.7, 1.75, 1.8, 2, 3, 4, 5, 6, 20, 100)) +\n  geom_point(aes(x = 0.5, y = -1), col = \"orange\", shape = \"cross\") +\n  geom_point(aes(x = -1.5, y = 1), col = \"orange\", shape = \"cross\") +\n  theme_minimal() +\n  labs(x = \"X1\", y = \"X2\",\n       title = \"Contour plot\")\n\n\n\n\nThe second-order Taylor expansion uses the first and second partial derivatives of the function \\(f(x)\\).\n\\[\n\\begin{aligned}\nf(x) &= e^{x1}(4e^2_1 + 2x^2_2 + 4x_1x_2 + 2x_2 + 1), \\\\\n\\frac{\\partial f}{\\partial x_1} &= f(x) + e^{x_1}(8x_1 + 4x_2), \\\\\n\\frac{\\partial f}{\\partial x_2} &= e^{x_1} (4x_2 + 4x_1 + 2), \\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} &= f(x) + 2e^{x1}(8x_1 + 4x_2) + 8e^{x_1}, \\\\\n\\frac{\\partial^2 f}{\\partial x_2^2} &= 4e^{x_1}, \\\\\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2).\n\\end{aligned}\n\\]\nAccordingly, the Gradient \\(\\nabla f(x)\\) is defined as\n\\[\n\\nabla f(x) =\n\\begin{pmatrix}\nf(x) + e^{x_1}(8x_1 + 4x_2) \\\\\ne^{x_1} (4x_2 + 4x_1 + 2)\n\\end{pmatrix},\n\\]\nand the Hessian \\(\\nabla^2 f(x)\\) is defined as\n\\[\n\\nabla^2 f(x) =\n\\begin{pmatrix}\nf(x) + 2e^{x1}(8x_1 + 4x_2) + 8e^{x_1} &\n4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2) \\\\\n4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2) &\n4e^{x_1}\n\\end{pmatrix}.\n\\]\nMoreover, the second-order Taylor series at \\(\\boldsymbol{x} = (0.5, -1)'\\) and \\(\\boldsymbol{x} = (-0.75, 1)'\\) is defined as\n\\[\n\\begin{aligned}\n\\nabla f((0.5, -1)) &=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\\n\\nabla^2 f((0.5, -1)) &=\n\\begin{pmatrix}\n13.19 & 6.59 \\\\\n6.59 & 6.59\n\\end{pmatrix},\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\nabla f((-1.5, 1)) &=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\\n\\nabla^2 f((-1.5, 1)) &=\n\\begin{pmatrix}\n0 & 0.89 \\\\\n0.89 & 0.89\n\\end{pmatrix}.\n\\end{aligned}\n\\]\nAs can be seen in the contour plot, the first point is a minimum, while the second point is a saddle point.\n3. Consider the likelihood function\n\\[\nL = \\prod^N_{i=1} \\frac{e^{(\\alpha + \\beta x_i)y_i}}{1 + e^{(\\alpha + \\beta x_i)}}.\n\\]\nderive the log-likelihood function, the gradient vector for the parameter vector \\(\\boldsymbol{\\theta} = (\\alpha, \\beta)\\) and the Hessian matrix for the parameter vector \\(\\boldsymbol{\\theta}\\).\nSolution\nThe log-likelihood is defined as\n\\[\n\\ell = \\sum_{i=1}^N (\\alpha + \\beta x_i)y_i - \\log(1 + e^{(\\alpha + \\beta x_i)}),\n\\]\ndifferentiation with respect to \\(\\alpha\\) yields\n\\[\n\\frac{\\partial \\ell}{\\partial \\alpha} =\n\\sum_{i=1}^N y_i - \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum_{i=1}^N y_i - \\pi_i,\n\\]\ndifferentiation with respect to \\(\\beta\\) yields\n\\[\n\\frac{\\partial \\ell}{\\partial \\beta} =\n\\sum_{i=1}^N y_i x_i - x_i \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum_{i=1}^N x_i(y_i - \\pi_i).\n\\]\nAccordingly, the gradient is defined as\n\\[\n\\nabla \\ell =\n\\begin{pmatrix}\n\\sum_{i=1}^N y_i - \\pi_i \\\\\n\\sum_{i=1}^N x_i(y_i - \\pi_i)\n\\end{pmatrix}.\n\\]\nThe second partial derivatives are defined as\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} &=\n\\sum_{i=1}^N - \\frac{e^{(\\alpha + \\beta x_i)}(1 + e^{(\\alpha + \\beta x_i)}) - e^{(\\alpha + \\beta x_i)}e^{(\\alpha + \\beta x_i)}}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} -\n\\frac{(e^{(\\alpha + \\beta x_i)})^2}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N \\pi_i(1 - \\pi_i), \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\beta^2} &=\n\\sum_{i=1}^N - x_i^2 \\frac{e^{(\\alpha + \\beta x_i)}(1 + e^{(\\alpha + \\beta x_i)}) - e^{(\\alpha + \\beta x_i)}e^{(\\alpha + \\beta x_i)}}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N x_i^2 \\pi_i(1 - \\pi_i), \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} &=\n\\sum_{i=1}^N - x_i \\frac{e^{(\\alpha + \\beta x_i)}(1 + e^{(\\alpha + \\beta x_i)}) - e^{(\\alpha + \\beta x_i)}e^{(\\alpha + \\beta x_i)}}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N x_i \\pi_i(1 - \\pi_i). \\\\\n\\end{aligned}\n\\]\nHence, the Hessian \\(\\nabla^2 \\ell\\) is defined as\n\\[\n\\nabla^2 \\ell =\n\\begin{pmatrix}\n- \\sum_{i=1}^N \\pi_i(1 - \\pi_i) & - \\sum_{i=1}^N x_i \\pi_i(1 - \\pi_i) \\\\\n- \\sum_{i=1}^N x_i \\pi_i(1 - \\pi_i) & - \\sum_{i=1}^N x_i^2 \\pi_i(1 - \\pi_i)\n\\end{pmatrix}.\n\\]\n4. Take the Weibull density\n\\[\np(y) = \\varphi \\rho y^{\\rho - 1}e^{-\\varphi y^\\rho}.\n\\]\nDerive the second-order Taylor series expansion of \\(p(y)\\) about \\(y = 1\\).\nSolution\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial y}\n\\Big[\n\\varphi \\rho y^{\\rho - 1}e^{-\\varphi y^\\rho}\n\\Big] &= \\varphi \\rho \\Big((\\rho - 1) y^{\\rho - 2} e^{-\\varphi y^\\rho} - \\varphi \\rho y^{2\\rho-2} e^{-\\varphi y^\\rho}\\Big) \\\\\n&= \\varphi \\rho e^{-\\varphi y^\\rho} y^{\\rho - 2}\\Big(\\rho - 1 - \\varphi \\rho y^\\rho\\Big), \\\\\n\\frac{\\partial^2}{\\partial y^2}\n\\Big[\n\\varphi \\rho y^{\\rho - 1}e^{-\\varphi y^\\rho}\n\\Big] &= \\varphi \\rho\n\\Bigg[\n\\frac{\\partial}{\\partial y} \\rho \\Big(e^{-\\varphi y^\\rho} y^{\\rho-2}\\Big) -\n\\frac{\\partial}{\\partial y} \\Big(e^{-\\varphi y^\\rho} y^{\\rho-2}\\Big) -\n\\frac{\\partial}{\\partial y} \\varphi \\rho\\Big(e^{-\\varphi y^\\rho} y^{2\\rho-2}\\Big)\n\\Bigg] \\\\\n&= \\varphi \\rho e^{-\\varphi y^\\rho} y^{\\rho-3}\n\\Big(\n(\\rho-1)(\\rho-2-\\varphi \\rho y^\\rho) - \\varphi \\rho y^rho(2\\rho - 2 - \\varphi \\rho y^rho)\n\\Big)\n\\end{aligned}\n\\]\n\nfx <- function(phi, rho, y) {\n  e <- exp(-phi*y^rho)\n  phi * rho * y^{rho-1} * e\n}\n\nfx1 <- function(phi, rho, y) {\n  e <- exp(-phi*y^rho)\n  phi*rho*e*y^{rho-2}*((rho-1) - phi*rho*y^rho)\n}\n \nfx2 <- function(phi, rho, y) {\n  e <- exp(-phi*y^rho)\n  phi*rho*e*y^{rho-3} * ((rho-1)*(rho-2-phi*rho*y^rho) - phi*rho*y^rho*(2*rho-2-phi*rho*y^rho))\n}\n\ntaylor <- function(phi, rho, y, root) {\n  fx(phi, rho, root) + fx1(phi, rho, root) * (y - root) + fx2(phi, rho, root)/2 * (y - root)^2\n}\n\nggplot() +\n  geom_function(fun = fx, args = list(phi = 1, rho = 2)) +\n  geom_function(fun = taylor, \n                args = list(phi = 1, rho = 2, root = 1),\n                col = \"orange\") + \n  lims(x = c(0, 3), y = c(0, 1)) +\n  theme_minimal() +\n  labs(x = expression(italic(y)), y = expression(italic(f(y))),\n       title = \"Second-order Taylor Series Expansion\")\n\n\n\n\n5. Consider the Weibull-based likelihood function:\n\\[\nL = \\prod^n_{i=1} \\rho y_i^{\\rho-1} e^{(\\alpha + \\beta x_i)} e^{-(y^\\rho_i e^{(\\alpha + \\beta x_i)})},\n\\]\nwith \\(y_i\\) the outcome (time-to-event), \\(x_i\\) is a continuous covariate, and \\(\\alpha\\) and \\(\\beta\\) are regression parameters. Derive the log-likelihood function for an i.i.d. sample of \\(n\\) observations \\((y_1, y_2, ..., y_n)\\), the gradient of the log-likelihood function for the parameters \\((\\rho, \\alpha, \\beta)\\) and the Hessian of the log-likelihood function for the parameter vector \\((\\rho, \\alpha, \\beta)\\).\nSolution\nThe log-likelihood is defined as\n\\[\n\\ell = \\sum^n_{i=1} \\log(\\rho) + (\\rho-1) \\log(y_i) + \\alpha + \\beta x_i - y_i^\\rho e^{(\\alpha + \\beta x_i)}.\n\\]\nThe first-order partial derivatives with respect to \\(\\rho, \\alpha, \\beta\\) are given by\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\rho} &=\n\\sum_{i=1}^n \\rho^{-1} + \\log(y_i) - y_i^\\rho e^{(\\alpha + \\beta x_i)} \\log(y_i), \\\\\n\\frac{\\partial \\ell}{\\partial \\alpha} &=\n\\sum_{i=1}^n 1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial \\ell}{\\partial \\alpha} &=\n\\sum_{i=1}^n x_i(1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}),\n\\end{aligned}\n\\]\nsuch that the gradient is defined as\n\\[\n\\nabla \\ell = \\begin{pmatrix}\n\\sum_{i=1}^n \\rho^{-1} + \\log(y_i) - y_i^\\rho e^{(\\alpha + \\beta x_i)} \\log(y_i), \\\\\n\\sum_{i=1}^n 1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\sum_{i=1}^n x_i(1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}),\n\\end{pmatrix}.\n\\]\nAdditionally, the second-order partial derivatives are defined by\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\rho^2} &=\n\\sum_{i=1}^n -\\rho^{-2} - y_i^\\rho e^{(\\alpha + \\beta x_i)} (\\log(y_i))^2, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} &=\n\\sum_{i=1}^n - y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\beta^2} &=\n\\sum_{i=1}^n - x_i^2y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\rho \\partial \\alpha} &=\n\\sum_{i=1}^n - \\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\rho \\partial \\beta} &=\n\\sum_{i=1}^n - x_i\\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} &=\n\\sum_{i=1}^n - x_i y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\end{aligned}\n\\]\nsuch that the Hessian is defined as\n\\[\n\\nabla^2 \\ell(\\rho, \\alpha, \\beta) =\n\\begin{pmatrix}\n\\sum_{i=1}^n -\\rho^{-2} - y_i^\\rho e^{(\\alpha + \\beta x_i)} (\\log(y_i))^2 \\\\\n\\sum_{i=1}^n - \\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\sum_{i=1}^n - y_i^\\rho e^{(\\alpha + \\beta x_i)} \\\\\n\\sum_{i=1}^n - x_i\\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\sum_{i=1}^n - x_i y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\sum_{i=1}^n - x_i^2y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\end{pmatrix}.\n\\]\n6. Consider a logistic regression\n\\[\n\\text{logit}[P(Y_i=1 | x_i)] = \\alpha + \\beta x_i,\n\\]\nand a small set of data\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(y_i\\)\n\n\n\n\n1\n0.5\n0\n\n\n2\n1.0\n0\n\n\n3\n1.5\n1\n\n\n4\n2.0\n0\n\n\n5\n2.5\n1\n\n\n\nConstruct the log-likelihood function and the gradient function.\nSolution\nConstructing the logit function requires an expression for \\(P(Y_i = 1 | x_i)\\), which is defined as follows.\n\\[\n\\begin{aligned}\n\\text{logit}[P(Y_i = 1 | x_i)] &= \\alpha + \\beta x_i, \\\\\n\\log \\Bigg(\\frac{P(Y_i = 1 | x_i)}{1 - P(Y_i = 1 | x_i)} \\Bigg) &= e^{(\\alpha + \\beta x_i)}, \\\\\nP(Y_i = 1 | x_i) &= e^{(\\alpha + \\beta x_i)} - e^{(\\alpha + \\beta x_i)}(P(Y_i=1|x_i)), \\\\\n1 &= \\frac{e^{(\\alpha + \\beta x_i)}}{P(Y_i = 1|x_i)} - e^{(\\alpha + \\beta x_i)}, \\\\\n1 + e^{(\\alpha + \\beta x_i)} &= \\frac{e^{(\\alpha + \\beta x_i)}}{P(Y_i = 1|x_i)}, \\\\\nP(Y_i = 1|x_i) &= \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}}.\n\\end{aligned}\n\\]\nPlugging this into a binomial likelihood function yields\n\\[\n\\begin{aligned}\nL &= \\prod^5_{i=1} \\pi_i^{y_i} (1 - \\pi_i)^{(1-y_i)}, \\\\\n\\ell &= \\sum_{i=1}^5 y_i \\log \\pi_i + (1 - y_i) \\log(1-\\pi_i) \\\\\n&= \\sum^5_{i=1} y_i \\log \\Bigg(\\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}}\\Bigg) +\n\\log \\Bigg(\\frac{1}{1 + e^{(\\alpha + \\beta x_i)}}\\Bigg) -\ny_i \\log \\Bigg(\\frac{1}{1 + e^{(\\alpha + \\beta x_i)}}\\Bigg) \\\\\n&= \\sum^5_{i=1} y_i \\log \\Bigg(\\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} \\Big/ \\frac{1}{1 + e^{(\\alpha + \\beta x_i)}} \\Bigg) + \\log \\Bigg(\\frac{1}{1 + e^{(\\alpha + \\beta x_i)}} \\Bigg) \\\\\n&= \\sum^n_{i=1} y_i (\\alpha + \\beta x_i) - \\log(1 + e^{(\\alpha + \\beta x_i)}).\n\\end{aligned}\n\\]\nAccordingly, we can define the Gradient as\n\\[\n\\nabla \\ell =\n\\begin{pmatrix}\n\\sum^5_{i=1} y_i - \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum^5_{i=1} y_i - \\pi_i \\\\\n\\sum^5_{i=1} y_i x_i - x_i\\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum^5_{i=1} x_i(y_i - \\pi_i).\n\\end{pmatrix}\n\\]\nFilling in the values for \\(y\\) yields\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\alpha} &= 2 - \\sum^5_{i=1} \\pi_i, \\\\\n\\frac{\\partial \\ell}{\\partial \\beta} &= 4 - \\sum^5_{i=1} x_i\\pi_i.\n\\end{aligned}\n\\]\n\nell <- function(x, y, alpha, beta) {\n  sum(y*(alpha + beta*x) - log(1 + exp(alpha + beta*x)))\n}\n\nx <- c(0.5, 1, 1.5, 2, 2.5)\ny <- c(0, 0, 1, 0, 1)\n\nexpand.grid(alpha = -5000:-2000/1000,\n            beta = 1000:4000/1000) |>\n  dplyr::mutate(l = purrr::map2_dbl(alpha, beta, ~ell(x, y, .x, .y))) |>\n  ggplot(aes(x = alpha, y = beta, z = l)) +\n  stat_contour_filled(bins = 100, show.legend = FALSE) +\n  theme_minimal() +\n  labs(x = expression(alpha),\n       y = expression(beta),\n       title = \"Contour plot of logistic regression log-likelihood\")\n\n\n\n\n7. Consider \\(f(x_1, x_2, x_3) = (x_1 - 1)^4 + (x_2 - 3)^2 + 4(x_3 + 5)^4\\). Find the Gradient and the Hessian and indicate what is special about the point \\((1, 3, -5)\\).\nSolution\nThe gradient is defined as\n\\[\n\\nabla f(x_1, x_2, x_3)= \\begin{pmatrix}\n4(x_1-1)^3 \\\\\n2(x_2-3) \\\\\n16(x_3+5)^3 \\\\\n\\end{pmatrix}.\n\\]\nThe Hessian is defined as\n\\[\n\\nabla^2 f(x_1, x_2, x_3) = \\begin{pmatrix}\n12(x_1-1)^2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 48(x_3+5)^2\n\\end{pmatrix}.\n\\]\nIn the point \\((1, 3, -5)\\), the Gradient is \\(\\nabla f(x_1, x_2, x_3) = (0,0,0)'\\), and the Hessian equals\n\\[\n\\nabla^2 f(x_1, x_2, x_3) = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n\\]\nIn the direction of \\(x_1\\) and \\(x_3\\), the function surface is almost flat."
  }
]