[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization and Numerical Methods Solutions",
    "section": "",
    "text": "This project has two purposes. First, it is an attempt to organize my solutions to the course Optimization and Numerical Methods in a structured way. Second, it provides a justification to try and learn Quarto."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Introduction",
    "section": "",
    "text": "No exercises."
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Motivating Problems",
    "section": "",
    "text": "Chapter 2 on motivating problems is the first chapter that actually entails exercises."
  },
  {
    "objectID": "chapter2.html#exercises-2.7-in-the-notes",
    "href": "chapter2.html#exercises-2.7-in-the-notes",
    "title": "2  Motivating Problems",
    "section": "2.1 Exercises (2.7 in the notes)",
    "text": "2.1 Exercises (2.7 in the notes)\n1. Consider the multinomial likelihood in Equation 2.1 for a model (for a two-way contingency table) assuming independence. Can you simplify the likelihood?\n\\[\n\\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk})~~~~~~~~~~~~~~~~ \\sum_{j=1}^R \\sum_{k=1}^C \\pi_{jk}=1\n\\tag{2.1}\\]\nSolution\n\\[\n\\begin{aligned}\n\\ell(\\pi) &= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{j+} \\cdot \\pi_{+k}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln \\pi_{j+} + n_{jk} \\ln \\pi_{+k} \\\\\n&= \\sum_{j=1}^R n_{j+} \\ln \\pi_{j+} + \\sum_{k=1}^C n_{+k} \\ln \\pi_{+k}\n\\end{aligned}\n\\tag{2.2}\\]\n2. In a mixed model, optimization is carried out using the marginal likelihood (the likelihood with the random effects integrated out). Define the marginal likelihood for the one-way random effects ANOVA model.\nOne-way random effects ANOVA with group-specific effects \\(\\mu_j \\sim \\mathcal{N}(0, \\sigma^2_{\\mu})\\), and\n\\[\ny_{ij} = \\beta + \\mu_j + \\epsilon_{ij},\n\\]\nwith \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2_\\epsilon)\\), with \\(a\\) groups indexed \\(j\\), and \\(n_j\\) individuals in every group.\nSolution\nSo, the likelihood consists of two components. For the individuals within each group, we have\n\\[\n\\prod^{n_j}_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg),\n\\]\nwhereas for the groups themselves, we have\n\\[\n\\prod^{a}_{j=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg).\n\\]\nCombining these components, and integrating out the random effects, we obtain the marginal likelihood\n\\[\n\\prod^{a}_{j=1}\n\\int\n\\prod^{n_j}_{i=1}\n\\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg)\n\\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg)\nd\\mu_j.\n\\]\n3. Suppose you do a simple linear regression analysis using a \\(t_\\nu\\)-distribution for the residuals (density: \\(f_\\nu(y) = C \\sqrt{\\lambda} \\Big(1 + \\frac{\\lambda(y-\\mu)^2}{\\nu}\\Big)^{-\\frac{\\nu+1}{2}}\\) where \\(\\mu\\) is the mean (for \\(\\nu > 1\\)), \\(\\lambda\\) is a scale parameter and \\(C\\) is a normalizing constraint that does not depend on \\(\\mu\\) or \\(\\lambda\\)). Define the (log-)likelihood for \\(n\\) observations \\((y_i, x_i)\\), such that \\(\\mu_i = \\beta_0 + \\beta_1x_i\\).\nSolution\n\\[\n\\begin{aligned}\nL(\\beta) &= \\prod_{i=1}^n C\\sqrt{\\lambda} \\Bigg(1 + \\frac{\\lambda (y_i-\\beta_0-\\beta_1x_i)^2}{\\nu}  \\Bigg)^{-\\frac{\\nu+1}{2}}, \\\\\n\\ell(\\beta) &= N \\ln C +  \\frac{N}{2} \\ln \\lambda - \\sum^n_{i=1} \\frac{\\nu + 1}{2} \\ln \\Bigg(1 + \\frac{\\lambda(y_i - \\beta_0 - \\beta_1x_i)^2}{\\nu}\\Bigg)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Basic tools",
    "section": "",
    "text": "Chapter 3 introduces basic tools for optimization problems, such as Taylor Series Expansion, and introduces the exponential family."
  },
  {
    "objectID": "chapter3.html#exercises-3.7-in-the-book",
    "href": "chapter3.html#exercises-3.7-in-the-book",
    "title": "3  Basic tools",
    "section": "3.1 Exercises (3.7 in the book)",
    "text": "3.1 Exercises (3.7 in the book)\n1. Consider \\(f(x) = \\frac{e^x}{1 + e^x}\\). Derive the third-order Taylor series expansion of this function at \\(x = 0\\), and make a graph with the function and the third-order Taylor series expansion at \\(x = 0\\).\nSolution\n\\[\n\\begin{aligned}\nf(x) &= \\frac{e^x}{1+e^x} \\\\\nf'(x) &= \\frac{e^x(1+e^x)}{(1+e^x)^2} - \\frac{e^{2x}}{(1+e^x)^2} = \\frac{e^x}{(1+e^x)^2} \\\\\nf''(x) &= \\frac{e^x(1+e^x)^2 - e^x 2(1 + e^x)e^x}{(1 + e^x)^4} \\\\\n&= \\frac{e^x(1+e^x)^2 - 2e^{2x}}{(1+e^x)^3} \\\\\n&= \\frac{e^x - e^{ 2x}}{(1+e^x)^3} \\\\\nf'''(x) &= \\frac{(e^x-2e^{2x})(1+e^x)^3 - (e^x - e^{2x}) 3 (1+e^x)^2e^x}{(1+e^x)^6} \\\\\n&= \\frac{e^x - 2e^{2x} + e^{2x} - 2e^{3x} - 3e^{2x} + 3e^{3x}}{(1+e^x)^4} \\\\\n&= \\frac{e^x - 4e^{2x} + e^{3x}}{(1+e^x)^4},\n\\end{aligned}\n\\]\nusing Taylor’s theorem, we get\n\\[\n\\begin{aligned}\nf(x) & \\approx \\sum^n_{k=0} \\frac{f^{(k)}(x_0)}{k!} (x - x_0)^k \\\\\n&= \\frac{1}{2} + \\frac{1}{4}x + 0 - \\frac{1}{48}x^3.\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\nfx <- function(x) exp(x) / (1 + exp(x))\nfx1 <- function(x) exp(x) / (1 + exp(x))^2\nfx2 <- function(x) (exp(x) - exp(2*x)) / (1 + exp(x))^3\nfx3 <- function(x) (exp(x) - 4*exp(2*x) + exp(3*x)) / (1 + exp(x))^4\n\ntaylor <- function(x, root) {\n  fx(root) + fx1(root) * (x - root) + fx2(root) / 2 * (x - root)^2 + fx3(root) / 6 * (x - root)^3\n}\n\nggplot() +\n  geom_function(fun = fx) +\n  geom_function(fun = taylor, args = list(root = 0), col = \"orange\") +\n  xlim(-3, 3) +\n  theme_minimal() +\n  labs(x = \"X\", y = expression(italic(f(X))),\n       title = \"Third-order Taylor Series Expansion\")\n\n\n\n\n2. Consider the function: \\(f(x) = e^{x_1}(4x_1^2 + 2x_2^2 + 4x_1x_2 + 2x_2 + 1)\\). Make a contour plot of this function (let both axes run from -2 to 2) at function values \\(0.2, 0.4, 0.7, 1, 1.7, 1.75, 1.8, 2, 3, 4, 5, 6, 20\\). Derive the second-order Taylor series at \\(\\boldsymbol{x} = (0.5, -1)'\\) and \\(\\boldsymbol{x} = (-0.75, 1)'\\).\nSolution\nContour plot\n\nfx12 <- function(x1, x2) {\n  exp(x1) * (4*x1^2 + 2*x2^2 + 4*x1*x2 + 2*x2 + 1)\n}\n\nexpand.grid(x1 = -200:200/100,\n            x2 = -200:200/100) |>\n  dplyr::mutate(z = fx12(x1, x2)) |>\n  ggplot(aes(x = x1, y = x2, z = z)) +\n  stat_contour_filled(breaks = c(0, 0.2, 0.4, 0.7, 1, 1.7, 1.75, 1.8, 2, 3, 4, 5, 6, 20, 100)) +\n  geom_point(aes(x = 0.5, y = -1), col = \"orange\", shape = \"cross\") +\n  geom_point(aes(x = -1.5, y = 1), col = \"orange\", shape = \"cross\") +\n  theme_minimal() +\n  labs(x = \"X1\", y = \"X2\",\n       title = \"Contour plot\")\n\n\n\n\nThe second-order Taylor expansion uses the first and second partial derivatives of the function \\(f(x)\\).\n\\[\n\\begin{aligned}\nf(x) &= e^{x1}(4e^2_1 + 2x^2_2 + 4x_1x_2 + 2x_2 + 1), \\\\\n\\frac{\\partial f}{\\partial x_1} &= f(x) + e^{x_1}(8x_1 + 4x_2), \\\\\n\\frac{\\partial f}{\\partial x_2} &= e^{x_1} (4x_2 + 4x_1 + 2), \\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} &= f(x) + 2e^{x1}(8x_1 + 4x_2) + 8e^{x_1}, \\\\\n\\frac{\\partial^2 f}{\\partial x_2^2} &= 4e^{x_1}, \\\\\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2).\n\\end{aligned}\n\\]\nAccordingly, the Gradient \\(\\nabla f(x)\\) is defined as\n\\[\n\\nabla f(x) =\n\\begin{pmatrix}\nf(x) + e^{x_1}(8x_1 + 4x_2) \\\\\ne^{x_1} (4x_2 + 4x_1 + 2)\n\\end{pmatrix},\n\\]\nand the Hessian \\(\\nabla^2 f(x)\\) is defined as\n\\[\n\\nabla^2 f(x) =\n\\begin{pmatrix}\nf(x) + 2e^{x1}(8x_1 + 4x_2) + 8e^{x_1} &\n4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2) \\\\\n4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2) &\n4e^{x_1}\n\\end{pmatrix}.\n\\]\nMoreover, the second-order Taylor series at \\(\\boldsymbol{x} = (0.5, -1)'\\) and \\(\\boldsymbol{x} = (-0.75, 1)'\\) is defined as\n\\[\n\\begin{aligned}\n\\nabla f((0.5, -1)) &=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\\n\\nabla^2 f((0.5, -1)) &=\n\\begin{pmatrix}\n13.19 & 6.59 \\\\\n6.59 & 6.59\n\\end{pmatrix},\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\nabla f((-1.5, 1)) &=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\\n\\nabla^2 f((-1.5, 1)) &=\n\\begin{pmatrix}\n0 & 0.89 \\\\\n0.89 & 0.89\n\\end{pmatrix}.\n\\end{aligned}\n\\]\nAs can be seen in the contour plot, the first point is a minimum, while the second point is a saddle point.\n3. Consider the likelihood function\n\\[\nL = \\prod^N_{i=1} \\frac{e^{(\\alpha + \\beta x_i)y_i}}{1 + e^{(\\alpha + \\beta x_i)}}.\n\\]\nderive the log-likelihood function, the gradient vector for the parameter vector \\(\\boldsymbol{\\theta} = (\\alpha, \\beta)\\) and the Hessian matrix for the parameter vector \\(\\boldsymbol{\\theta}\\).\nSolution\nThe log-likelihood is defined as\n\\[\n\\ell = \\sum_{i=1}^N (\\alpha + \\beta x_i)y_i - \\log(1 + e^{(\\alpha + \\beta x_i)}),\n\\]\ndifferentiation with respect to \\(\\alpha\\) yields\n\\[\n\\frac{\\partial \\ell}{\\partial \\alpha} =\n\\sum_{i=1}^N y_i - \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum_{i=1}^N y_i - \\pi_i,\n\\]\ndifferentiation with respect to \\(\\beta\\) yields\n\\[\n\\frac{\\partial \\ell}{\\partial \\beta} =\n\\sum_{i=1}^N y_i x_i - x_i \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum_{i=1}^N x_i(y_i - \\pi_i).\n\\]\nAccordingly, the gradient is defined as\n\\[\n\\nabla \\ell =\n\\begin{pmatrix}\n\\sum_{i=1}^N y_i - \\pi_i \\\\\n\\sum_{i=1}^N x_i(y_i - \\pi_i)\n\\end{pmatrix}.\n\\]\nThe second partial derivatives are defined as\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} &=\n\\sum_{i=1}^N - \\frac{e^{(\\alpha + \\beta x_i)}(1 + e^{(\\alpha + \\beta x_i)}) - e^{(\\alpha + \\beta x_i)}e^{(\\alpha + \\beta x_i)}}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} -\n\\frac{(e^{(\\alpha + \\beta x_i)})^2}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N \\pi_i(1 - \\pi_i), \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\beta^2} &=\n\\sum_{i=1}^N - x_i^2 \\frac{e^{(\\alpha + \\beta x_i)}(1 + e^{(\\alpha + \\beta x_i)}) - e^{(\\alpha + \\beta x_i)}e^{(\\alpha + \\beta x_i)}}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N x_i^2 \\pi_i(1 - \\pi_i), \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} &=\n\\sum_{i=1}^N - x_i \\frac{e^{(\\alpha + \\beta x_i)}(1 + e^{(\\alpha + \\beta x_i)}) - e^{(\\alpha + \\beta x_i)}e^{(\\alpha + \\beta x_i)}}{(1 + e^{(\\alpha + \\beta x_i)})^2} \\\\\n&= - \\sum_{i=1}^N x_i \\pi_i(1 - \\pi_i). \\\\\n\\end{aligned}\n\\]\nHence, the Hessian \\(\\nabla^2 \\ell\\) is defined as\n\\[\n\\nabla^2 \\ell =\n\\begin{pmatrix}\n- \\sum_{i=1}^N \\pi_i(1 - \\pi_i) & - \\sum_{i=1}^N x_i \\pi_i(1 - \\pi_i) \\\\\n- \\sum_{i=1}^N x_i \\pi_i(1 - \\pi_i) & - \\sum_{i=1}^N x_i^2 \\pi_i(1 - \\pi_i)\n\\end{pmatrix}.\n\\]\n4. Take the Weibull density\n\\[\np(y) = \\varphi \\rho y^{\\rho - 1}e^{-\\varphi y^\\rho}.\n\\]\nDerive the second-order Taylor series expansion of \\(p(y)\\) about \\(y = 1\\).\nSolution\n\\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial y}\n\\Big[\n\\varphi \\rho y^{\\rho - 1}e^{-\\varphi y^\\rho}\n\\Big] &= \\varphi \\rho \\Big((\\rho - 1) y^{\\rho - 2} e^{-\\varphi y^\\rho} - \\varphi \\rho y^{2\\rho-2} e^{-\\varphi y^\\rho}\\Big) \\\\\n&= \\varphi \\rho e^{-\\varphi y^\\rho} y^{\\rho - 2}\\Big(\\rho - 1 - \\varphi \\rho y^\\rho\\Big), \\\\\n\\frac{\\partial^2}{\\partial y^2}\n\\Big[\n\\varphi \\rho y^{\\rho - 1}e^{-\\varphi y^\\rho}\n\\Big] &= \\varphi \\rho\n\\Bigg[\n\\frac{\\partial}{\\partial y} \\rho \\Big(e^{-\\varphi y^\\rho} y^{\\rho-2}\\Big) -\n\\frac{\\partial}{\\partial y} \\Big(e^{-\\varphi y^\\rho} y^{\\rho-2}\\Big) -\n\\frac{\\partial}{\\partial y} \\varphi \\rho\\Big(e^{-\\varphi y^\\rho} y^{2\\rho-2}\\Big)\n\\Bigg] \\\\\n&= \\varphi \\rho e^{-\\varphi y^\\rho} y^{\\rho-3}\n\\Big(\n(\\rho-1)(\\rho-2-\\varphi \\rho y^\\rho) - \\varphi \\rho y^rho(2\\rho - 2 - \\varphi \\rho y^rho)\n\\Big)\n\\end{aligned}\n\\]\n\nfx <- function(phi, rho, y) {\n  e <- exp(-phi*y^rho)\n  phi * rho * y^{rho-1} * e\n}\n\nfx1 <- function(phi, rho, y) {\n  e <- exp(-phi*y^rho)\n  phi*rho*e*y^{rho-2}*((rho-1) - phi*rho*y^rho)\n}\n \nfx2 <- function(phi, rho, y) {\n  e <- exp(-phi*y^rho)\n  phi*rho*e*y^{rho-3} * ((rho-1)*(rho-2-phi*rho*y^rho) - phi*rho*y^rho*(2*rho-2-phi*rho*y^rho))\n}\n\ntaylor <- function(phi, rho, y, root) {\n  fx(phi, rho, root) + fx1(phi, rho, root) * (y - root) + fx2(phi, rho, root)/2 * (y - root)^2\n}\n\nggplot() +\n  geom_function(fun = fx, args = list(phi = 1, rho = 2)) +\n  geom_function(fun = taylor, \n                args = list(phi = 1, rho = 2, root = 1),\n                col = \"orange\") + \n  lims(x = c(0, 3), y = c(0, 1)) +\n  theme_minimal() +\n  labs(x = expression(italic(y)), y = expression(italic(f(y))),\n       title = \"Second-order Taylor Series Expansion\")\n\n\n\n\n5. Consider the Weibull-based likelihood function:\n\\[\nL = \\prod^n_{i=1} \\rho y_i^{\\rho-1} e^{(\\alpha + \\beta x_i)} e^{-(y^\\rho_i e^{(\\alpha + \\beta x_i)})},\n\\]\nwith \\(y_i\\) the outcome (time-to-event), \\(x_i\\) is a continuous covariate, and \\(\\alpha\\) and \\(\\beta\\) are regression parameters. Derive the log-likelihood function for an i.i.d. sample of \\(n\\) observations \\((y_1, y_2, ..., y_n)\\), the gradient of the log-likelihood function for the parameters \\((\\rho, \\alpha, \\beta)\\) and the Hessian of the log-likelihood function for the parameter vector \\((\\rho, \\alpha, \\beta)\\).\nSolution\nThe log-likelihood is defined as\n\\[\n\\ell = \\sum^n_{i=1} \\log(\\rho) + (\\rho-1) \\log(y_i) + \\alpha + \\beta x_i - y_i^\\rho e^{(\\alpha + \\beta x_i)}.\n\\]\nThe first-order partial derivatives with respect to \\(\\rho, \\alpha, \\beta\\) are given by\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\rho} &=\n\\sum_{i=1}^n \\rho^{-1} + \\log(y_i) - y_i^\\rho e^{(\\alpha + \\beta x_i)} \\log(y_i), \\\\\n\\frac{\\partial \\ell}{\\partial \\alpha} &=\n\\sum_{i=1}^n 1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial \\ell}{\\partial \\alpha} &=\n\\sum_{i=1}^n x_i(1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}),\n\\end{aligned}\n\\]\nsuch that the gradient is defined as\n\\[\n\\nabla \\ell = \\begin{pmatrix}\n\\sum_{i=1}^n \\rho^{-1} + \\log(y_i) - y_i^\\rho e^{(\\alpha + \\beta x_i)} \\log(y_i), \\\\\n\\sum_{i=1}^n 1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\sum_{i=1}^n x_i(1 - y_i^\\rho e^{(\\alpha + \\beta x_i)}),\n\\end{pmatrix}.\n\\]\nAdditionally, the second-order partial derivatives are defined by\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\rho^2} &=\n\\sum_{i=1}^n -\\rho^{-2} - y_i^\\rho e^{(\\alpha + \\beta x_i)} (\\log(y_i))^2, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\alpha^2} &=\n\\sum_{i=1}^n - y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\beta^2} &=\n\\sum_{i=1}^n - x_i^2y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\rho \\partial \\alpha} &=\n\\sum_{i=1}^n - \\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\rho \\partial \\beta} &=\n\\sum_{i=1}^n - x_i\\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\alpha \\partial \\beta} &=\n\\sum_{i=1}^n - x_i y_i^\\rho e^{(\\alpha + \\beta x_i)}, \\\\\n\\end{aligned}\n\\]\nsuch that the Hessian is defined as\n\\[\n\\nabla^2 \\ell(\\rho, \\alpha, \\beta) =\n\\begin{pmatrix}\n\\sum_{i=1}^n -\\rho^{-2} - y_i^\\rho e^{(\\alpha + \\beta x_i)} (\\log(y_i))^2 \\\\\n\\sum_{i=1}^n - \\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\sum_{i=1}^n - y_i^\\rho e^{(\\alpha + \\beta x_i)} \\\\\n\\sum_{i=1}^n - x_i\\log(y_i) y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\sum_{i=1}^n - x_i y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\sum_{i=1}^n - x_i^2y_i^\\rho e^{(\\alpha + \\beta x_i)} &\n\\end{pmatrix}.\n\\]\n6. Consider a logistic regression\n\\[\n\\text{logit}[P(Y_i=1 | x_i)] = \\alpha + \\beta x_i,\n\\]\nand a small set of data\n\n\n\n\\(i\\)\n\\(x_i\\)\n\\(y_i\\)\n\n\n\n\n1\n0.5\n0\n\n\n2\n1.0\n0\n\n\n3\n1.5\n1\n\n\n4\n2.0\n0\n\n\n5\n2.5\n1\n\n\n\nConstruct the log-likelihood function and the gradient function.\nSolution\nConstructing the logit function requires an expression for \\(P(Y_i = 1 | x_i)\\), which is defined as follows.\n\\[\n\\begin{aligned}\n\\text{logit}[P(Y_i = 1 | x_i)] &= \\alpha + \\beta x_i, \\\\\n\\log \\Bigg(\\frac{P(Y_i = 1 | x_i)}{1 - P(Y_i = 1 | x_i)} \\Bigg) &= e^{(\\alpha + \\beta x_i)}, \\\\\nP(Y_i = 1 | x_i) &= e^{(\\alpha + \\beta x_i)} - e^{(\\alpha + \\beta x_i)}(P(Y_i=1|x_i)), \\\\\n1 &= \\frac{e^{(\\alpha + \\beta x_i)}}{P(Y_i = 1|x_i)} - e^{(\\alpha + \\beta x_i)}, \\\\\n1 + e^{(\\alpha + \\beta x_i)} &= \\frac{e^{(\\alpha + \\beta x_i)}}{P(Y_i = 1|x_i)}, \\\\\nP(Y_i = 1|x_i) &= \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}}.\n\\end{aligned}\n\\]\nPlugging this into a binomial likelihood function yields\n\\[\n\\begin{aligned}\nL &= \\prod^5_{i=1} \\pi_i^{y_i} (1 - \\pi_i)^{(1-y_i)}, \\\\\n\\ell &= \\sum_{i=1}^5 y_i \\log \\pi_i + (1 - y_i) \\log(1-\\pi_i) \\\\\n&= \\sum^5_{i=1} y_i \\log \\Bigg(\\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}}\\Bigg) +\n\\log \\Bigg(\\frac{1}{1 + e^{(\\alpha + \\beta x_i)}}\\Bigg) -\ny_i \\log \\Bigg(\\frac{1}{1 + e^{(\\alpha + \\beta x_i)}}\\Bigg) \\\\\n&= \\sum^5_{i=1} y_i \\log \\Bigg(\\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} \\Big/ \\frac{1}{1 + e^{(\\alpha + \\beta x_i)}} \\Bigg) + \\log \\Bigg(\\frac{1}{1 + e^{(\\alpha + \\beta x_i)}} \\Bigg) \\\\\n&= \\sum^n_{i=1} y_i (\\alpha + \\beta x_i) - \\log(1 + e^{(\\alpha + \\beta x_i)}).\n\\end{aligned}\n\\]\nAccordingly, we can define the Gradient as\n\\[\n\\nabla \\ell =\n\\begin{pmatrix}\n\\sum^5_{i=1} y_i - \\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum^5_{i=1} y_i - \\pi_i \\\\\n\\sum^5_{i=1} y_i x_i - x_i\\frac{e^{(\\alpha + \\beta x_i)}}{1 + e^{(\\alpha + \\beta x_i)}} =\n\\sum^5_{i=1} x_i(y_i - \\pi_i).\n\\end{pmatrix}\n\\]\nFilling in the values for \\(y\\) yields\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\alpha} &= 2 - \\sum^5_{i=1} \\pi_i, \\\\\n\\frac{\\partial \\ell}{\\partial \\beta} &= 4 - \\sum^5_{i=1} x_i\\pi_i.\n\\end{aligned}\n\\]\n\nell <- function(x, y, alpha, beta) {\n  sum(y*(alpha + beta*x) - log(1 + exp(alpha + beta*x)))\n}\n\nx <- c(0.5, 1, 1.5, 2, 2.5)\ny <- c(0, 0, 1, 0, 1)\n\nexpand.grid(alpha = -5000:-2000/1000,\n            beta = 1000:4000/1000) |>\n  dplyr::mutate(l = purrr::map2_dbl(alpha, beta, ~ell(x, y, .x, .y))) |>\n  ggplot(aes(x = alpha, y = beta, z = l)) +\n  stat_contour_filled(bins = 50, show.legend = FALSE) +\n  theme_minimal() +\n  labs(x = expression(alpha),\n       y = expression(beta),\n       title = \"Contour plot of logistic regression log-likelihood\")\n\n\n\n\n7. Consider \\(f(x_1, x_2, x_3) = (x_1 - 1)^4 + (x_2 - 3)^2 + 4(x_3 + 5)^4\\). Find the Gradient and the Hessian and indicate what is special about the point \\((1, 3, -5)\\).\nSolution\nThe gradient is defined as\n\\[\n\\nabla f(x_1, x_2, x_3)= \\begin{pmatrix}\n4(x_1-1)^3 \\\\\n2(x_2-3) \\\\\n16(x_3+5)^3 \\\\\n\\end{pmatrix}.\n\\]\nThe Hessian is defined as\n\\[\n\\nabla^2 f(x_1, x_2, x_3) = \\begin{pmatrix}\n12(x_1-1)^2 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 48(x_3+5)^2\n\\end{pmatrix}.\n\\]\nIn the point \\((1, 3, -5)\\), the Gradient is \\(\\nabla f(x_1, x_2, x_3) = (0,0,0)'\\), and the Hessian equals\n\\[\n\\nabla^2 f(x_1, x_2, x_3) = \\begin{pmatrix}\n0 & 0 & 0 \\\\\n0 & 2 & 0 \\\\\n0 & 0 & 0\n\\end{pmatrix}.\n\\]\nIn the direction of \\(x_1\\) and \\(x_3\\), the function surface is almost flat."
  },
  {
    "objectID": "chapter4.html",
    "href": "chapter4.html",
    "title": "4  From non-iterative to iterative procedures",
    "section": "",
    "text": "No exercises."
  },
  {
    "objectID": "chapter5.html",
    "href": "chapter5.html",
    "title": "5  Least squares",
    "section": "",
    "text": "TO DO."
  },
  {
    "objectID": "chapter6.html",
    "href": "chapter6.html",
    "title": "6  Iteration-based Function Optimization",
    "section": "",
    "text": "Chapter 2 on motivating problems is the first chapter that actually entails exercises."
  },
  {
    "objectID": "chapter6.html#exercises-6.5-in-the-notes",
    "href": "chapter6.html#exercises-6.5-in-the-notes",
    "title": "6  Iteration-based Function Optimization",
    "section": "6.1 Exercises (6.5 in the notes)",
    "text": "6.1 Exercises (6.5 in the notes)\n1. Suppose for every individual in a small pre-clinical study, it has been recorded how many epileptic seizures are observed (outcome \\(y\\)) and whether the individual is receiving a standard treatment (covariate \\(x=0\\)) or experimental medication (covariate \\(x=1\\)). The data are:\n\n\n\nSubject \\(i\\)\nTreatment \\(x\\)\n# Seizures \\(y\\)\n\n\n\n\n1\n1\n12\n\n\n2\n1\n15\n\n\n3\n1\n17\n\n\n4\n0\n8\n\n\n5\n0\n11\n\n\n6\n0\n5\n\n\n\nA Poisson regression model is put forward for these data, with linear predictor \\(\\theta_i = \\beta_0 + \\beta_1 x_i\\). Starting from \\(\\boldsymbol{\\beta}^{(0)} = (0,0)'\\), do the following: Derive the likelihood equations. Can they be solved analytically in this case? Perform the first five steps of the Newton-Raphson algorithm to find the maximum of the likelihood. Put your results in a table with as columns: Iteration number, current point, and log-likelihood value. Do the same for Fisher-scoring.\nSolution\nThe Poisson model yields\n\\[\nY \\sim \\text{Poisson}(\\lambda),\n~ \\text{with} ~\nf(y|\\theta, \\phi) = \\frac{e^{-\\lambda}\\lambda^y}{y!},\n\\]\nand thus the likelihood \\(L\\) and log-likelihood \\(\\ell\\) are defined as\n\\[\n\\begin{aligned}\nL &= \\prod^6_{i=1} \\frac{e^{-\\lambda}\\lambda^{y_i}}{y_i!} = \\frac{e^{-e^{(\\beta_0 + \\beta_1 x_i)}} e^{(\\beta_0 \\beta_1 x_i)y_i}}{y_i!} \\\\\n\\ell &= \\sum^6_{i=1} y_i \\log \\lambda - \\lambda - \\log (y_i!) \\\\\n&= \\sum^6_{i=1} y_i(\\beta_0 + \\beta_1 x_i) - e^{(\\beta_0 + \\beta_1 x_i)} - \\log (y_i!).\n\\end{aligned}\n\\]\nAccordingly, the first-order partial derivatives are defined as\n\\[\n\\begin{aligned}\n\\frac{\\partial \\ell}{\\partial \\beta_0} &=\n\\sum^6_{i=1} y_i - e^{(\\beta_0 - \\beta_1 x_i)}, \\\\\n\\frac{\\partial \\ell}{\\partial \\beta_1} &=\n\\sum^6_{i=1} x_i y_i - x_i e^{(\\beta_0 - \\beta_1 x_i)},\n\\end{aligned}\n\\]\nand hence the Gradient (i.e., Score equation) can be written as\n\\[\n\\nabla \\ell(\\beta_0, \\beta_1) = S(\\theta) = \\begin{pmatrix}\n\\sum^6_{i=1} y_i - e^{(\\beta_0 - \\beta_1 x_i)}, \\\\\n\\sum^6_{i=1} x_i (y_i - e^{(\\beta_0 - \\beta_1 x_i)}).\n\\end{pmatrix}\n\\]\nAdditionally, the second-order partial derivates are defined as\n\\[\n\\begin{aligned}\n\\frac{\\partial^2 \\ell}{\\partial \\beta_0^2} &=\n\\sum^6_{i=1} - e^{(\\beta_0 - \\beta_1 x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\beta_1^2} &=\n\\sum^6_{i=1} - x_i^2 e^{(\\beta_0 - \\beta_1 x_i)}, \\\\\n\\frac{\\partial^2 \\ell}{\\partial \\beta_0 \\partial \\beta_1} &=\n\\sum^6_{i=1} - x_i e^{(\\beta_0 - \\beta_1 x_i)}, \\\\\n\\end{aligned}\n\\]\nsuch that the Hessian \\(\\nabla^2 \\ell(\\beta_0, \\beta_1)\\) can be written as\n\\[\n\\nabla^2 \\ell(\\beta_0, \\beta_1) =\n\\begin{pmatrix}\n\\sum^6_{i=1} - e^{(\\beta_0 - \\beta_1 x_i)} & \\\\\n\\sum^6_{i=1} - x_i e^{(\\beta_0 - \\beta_1 x_i)} &\n\\sum^6_{i=1} - x_i^2 e^{(\\beta_0 - \\beta_1 x_i)}\n\\end{pmatrix}.\n\\]\nSetting the first-order partial derivatives to zero and filling in the data yields\n\\[\nS(\\theta) = \\begin{pmatrix}\n68 - 3e^{(\\beta_0 + \\beta_1)} - 3e^{(\\beta_0)} \\\\\n44 - 3e^{(\\beta_0 + \\beta_1)}\n\\end{pmatrix} = \\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix}.\n\\]\nHence, we have\n\\[\n\\begin{aligned}\n44 - 3e^{(\\beta_0 + \\beta_1)} &= 0 \\\\\n3e^{(\\beta_0 + \\beta_1)} &= 44,\n\\end{aligned}\n\\]\nand thus\n\\[\n\\begin{aligned}\n68 - 3e^{(\\beta_0)} &= 44 \\\\\n3e^{(\\beta_0)} &= 24 \\\\\ne^{(\\beta_0)} &= 8 \\\\\n\\beta_0 &= \\log 8 \\approx 2.0794.\n\\end{aligned}\n\\]\nFilling this into the previous equation yields\n\\[\n\\begin{aligned}\n3e^{(\\log 8 + \\beta_1)} &= 44 \\\\\n\\log 44 - \\log 3 - \\log 8 &= \\beta_1 \\approx 0.6061.\n\\end{aligned}\n\\]\nNewton-Raphson method\n\nNR <- function(formula, data = NULL, start, n.iter) {\n  \n  X <- model.matrix(formula, data)\n  Y <- model.frame(formula, data)[,1]\n  \n  loglikelihood <- function(X, Y, beta) {\n    constant <- sapply(Y, function(y) sum(log(1:y))) |> sum()\n    sum(y - X %*% beta - exp(X %*% beta) - constant)\n  }\n  \n  score <- function(X, Y, beta) {\n    t(X) %*% (Y - exp(X %*% beta))\n  }\n  \n  hess <- function(X, beta) {\n    - t(X) %*% diag(c(exp(X %*% beta))) %*% X\n  }\n  \n  out <- matrix(0, n.iter+1, ncol(X))\n  out[1, ] <- b <- start\n  \n  logL <- numeric(n.iter+1)\n  logL[1] <- loglikelihood(X, Y, b)\n  \n  for (i in (1:n.iter)+1) {\n    b <- b - solve(hess(X, b)) %*% score(X, Y, b)\n    out[i, ] <- b\n    logL[i] <- loglikelihood(X, Y, b)\n  }\n  data.frame(iter = 0:n.iter,\n             out,\n             logL = logL)\n}\n\nx <- c(1, 1, 1, 0, 0, 0)\ny <- c(12, 15, 17, 8, 11, 5)\n\nNR(y ~ x, start = c(0,0), n.iter = 20) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    X1 \n    X2 \n    logL \n  \n \n\n  \n    0 \n    0.000000 \n    0.0000000 \n    -623.7158 \n  \n  \n    1 \n    7.000000 \n    6.6666667 \n    -2589080.4867 \n  \n  \n    2 \n    6.007295 \n    6.6593886 \n    -952918.2151 \n  \n  \n    3 \n    5.026981 \n    6.6397490 \n    -351004.0668 \n  \n  \n    4 \n    4.079450 \n    6.5874061 \n    -129568.7184 \n  \n  \n    5 \n    3.214784 \n    6.4524137 \n    -48104.1538 \n  \n  \n    6 \n    2.536096 \n    6.1320304 \n    -18133.0024 \n  \n  \n    7 \n    2.169495 \n    5.5011536 \n    -7106.9141 \n  \n  \n    8 \n    2.083377 \n    4.5941106 \n    -3051.0568 \n  \n  \n    9 \n    2.079449 \n    3.6165031 \n    -1558.0226 \n  \n  \n    10 \n    2.079442 \n    2.6657840 \n    -1007.2910 \n  \n  \n    11 \n    2.079442 \n    1.7932829 \n    -803.7918 \n  \n  \n    12 \n    2.079442 \n    1.0983733 \n    -729.4703 \n  \n  \n    13 \n    2.079442 \n    0.7096305 \n    -705.1191 \n  \n  \n    14 \n    2.079442 \n    0.6113113 \n    -700.2547 \n  \n  \n    15 \n    2.079442 \n    0.6061492 \n    -700.0115 \n  \n  \n    16 \n    2.079442 \n    0.6061358 \n    -700.0108 \n  \n  \n    17 \n    2.079442 \n    0.6061358 \n    -700.0108 \n  \n  \n    18 \n    2.079442 \n    0.6061358 \n    -700.0108 \n  \n  \n    19 \n    2.079442 \n    0.6061358 \n    -700.0108 \n  \n  \n    20 \n    2.079442 \n    0.6061358 \n    -700.0108 \n  \n\n\n\n\n\nFisher scoring\nNote that in this case, the expected Hessian equals\n\\[\n- x_i' \\frac{\\partial \\mu_i}{\\partial \\theta_i} \\nu_i^{-1} \\frac{\\partial \\mu_i}{\\partial \\theta_i} x_i.\n\\]\nGiven that\n\\[\n\\frac{\\partial \\mu_i}{\\partial \\theta_i} = \\frac{\\partial \\mu_i}{\\partial \\theta_i}\n\\Bigg(\\exp\\{\\theta_i\\}\\Bigg) = \\exp{\\theta_i},\n\\]\nand\n\\[\n\\nu_i^{-1} = \\frac{1}{\\exp\\{\\theta_i\\}},\n\\]\nit follows that\n\\[\n\\frac{\\partial \\mu_i}{\\partial \\theta_i} \\nu_i^{-1} \\frac{\\partial \\mu_i}{\\partial \\theta_i} = \\exp\\{\\theta_i\\} = \\exp\\{X\\beta\\}.\n\\]\nHence, for the expected Hessian, we have\n\\[\n\\mathcal{H} =\nE\\Bigg(\\frac{\\partial^2 \\ell}{\\partial \\beta \\partial \\beta'}\\Bigg) =\nX' \\text{diag}(\\exp{X\\beta})X,\n\\]\nwhich is equal to the Hessian matrix \\(H(\\beta)\\), and thus Fisher scoring and Newton-Raphson are equivalent in this case.\n2. Assume the function\n\\[\nf(x_1, x_2) = 8x_1 + 12x_2 + x_1^2 - 2x_2^2.\n\\]\nSketch the contour lines of \\(f(x_1, x_2)\\), and find the stationary point of \\(f(x_1, x_2)\\). Does this point correspond to a minimum, a maximum, or something else?\nSolution\n\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(purrr)\n\nfx1x2 <- function(x1, x2) 8*x1 + 12*x2 + x1^2 - 2*x2^2\n\nexpand.grid(x1 = -300:200/10,\n            x2 = -200:300/10) |>\n  mutate(f = map2_dbl(x1, x2, fx1x2)) |>\n  ggplot(aes(x = x1, y = x2, z = f)) +\n  stat_contour_filled(bins = 30, show.legend = FALSE) +\n  theme_minimal() +\n  labs(x = expression(italic(X[1])),\n       y = expression(italic(X[2])),\n       title = \"Contour plot\")\n\n\n\n\nThe first- and second-order partial derivatives of \\(f(x_1, x_2)\\) are given by\n\\[\n\\begin{aligned}\nf(x1, x2) &= 8x_1 + 12x_2 + x_1^2 - 2x_2^2, \\\\\n\\frac{\\partial f}{\\partial x_1} &= 8 + 2x_1, \\\\\n\\frac{\\partial f}{\\partial x_2} &= 12 - 4x_2, \\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} &= 2, \\\\\n\\frac{\\partial^2 f}{\\partial x_2^2} &= -4, \\\\\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 0. \\\\\n\\end{aligned}\n\\]\nThe stationary point of \\(f(x_1, x_2)\\) is \\(f(-4, 3)\\), which is a saddle point.\n3. Consider the function\n\\[\nf(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.\n\\]\nShow that (1, 1)’ is a local minimizer of this function. Also, starting from the point \\(\\boldsymbol{x}^{(0)}=(0,0)'\\), perform the first five steps of the steepest descent and the Newton-Raphson algorithm to minimize the function. Put your results in a table with as columns: iteration number, current point, function value and gradient.\nSolution\n\nfx1x2 <- function(x1, x2) 100*(x2 - x1^2)^2 + (1 - x1)^2\n\nexpand.grid(x1 = -100:200/100,\n            x2 = -100:200/100) |>\n  mutate(f = map2_dbl(x1, x2, fx1x2)) |>\n  ggplot(aes(x = x1, y = x2, z = f)) +\n  stat_contour_filled(bins = 50, show.legend = FALSE) +\n  theme_minimal() +\n  labs(x = expression(italic(X[1])),\n       y = expression(italic(X[2])),\n       title = \"Contour plot\")\n\n\n\n\nShowing that the point \\((1,1)'\\) is a local minimizer can be done by plugging the \\((1,1)'\\) into the Gradient, and checking whether the Gradient equals zero,\n\\[\n\\begin{aligned}\nf(x_1, x_2) &= 100(x_2 - x_1^2)^2 + (1 - x_1)^2, \\\\\n\\nabla f(x_1, x_2) &= \\begin{pmatrix}\n-400x_1(x_2 - x_1^2) - 2(1 - x_1) \\\\\n200(x_2 - x_1^2) \\end{pmatrix}, \\\\\n\\nabla f(1,1) &= \\begin{pmatrix}\n-400(1 - 1) - 2(1 - 1) \\\\\n200(1 - 1) \\end{pmatrix} =\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix},\n\\end{aligned}\n\\]\nwhich shows that \\((1,1)'\\) is a local minimizer. Moreover, the Hessian matrix is defined by\n\\[\n\\nabla^2 f(x_1, x_2) = \\begin{pmatrix}\n1200x_1^2 - 400x_2 + 2 & -400x_1 \\\\\n-400x_1 & 200 \\end{pmatrix}\n\\]\nSteepest-Descent\n\nf <- function(x1, x2) 100*(x2 - x1^2)^2 + (1 - x1)^2\n\nscore <- function(x1, x2) {\n  c(400*x1^3 - 400*x1*x2 + 2*x1 - 2,\n    200*x2 - 200*x1^2)\n}\n  \nhess <- function(x1, x2) {\n  matrix(c(1200*x1^2 - 400*x2 + 2, -400*x1, -400*x1, 200), \n         nrow = 2, ncol = 2)\n}\n\nSD <- function(start, n.iter, alpha, rho, tol = 1e-16) {\n  b <- start\n  grad <- matrix(0, n.iter + 1, 2)\n  grad[1, ] <- score(b[1], b[2])\n  \n  out <- matrix(0, n.iter + 1, 2)\n  out[1, ] <- b\n  \n  i <- 1; conv <- FALSE\n  \n  while (!conv) {\n    i <- i+1\n    fold <- f(out[i-1, 1], out[i-1, 2])\n    gradvec <- score(out[i-1, 1], out[i-1, 2])\n    out[i, ] <- out[i-1, ] - alpha * gradvec / sum(gradvec^2)\n    grad[i, ] <- gradvec\n    \n    fnew <- f(out[i,1], out[i,2])\n    a <- alpha\n    while(fnew > fold) {\n      a <- a*rho\n      out[i, ] <- out[i-1, ] - a * gradvec / sum(gradvec^2)\n      grad[i, ] <- c(score(out[i,1], out[i,2]))\n      fnew <- f(out[i,1], out[i,2])\n    }\n    if (\n      i - 1 == n.iter | \n      abs(fnew - fold) < tol\n    ) {\n      conv <- TRUE\n    }\n    \n  }\n  data.frame(iter = 0:(nrow(out)-1), \n             out  = out, \n             grad = grad, \n             fval = f(out[,1], out[,2])) |>\n    subset(iter < i)\n}\n\nSDout <- SD(c(0,0), 20000, 1, 0.8, 1e-10)\n\nSDout |>\n  head(15) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    out.1 \n    out.2 \n    grad.1 \n    grad.2 \n    fval \n  \n \n\n  \n    0 \n    0.0000000 \n    0.0000000 \n    -2.0000000 \n    0.000000 \n    1.0000000 \n  \n  \n    1 \n    0.2560000 \n    0.0000000 \n    5.2228864 \n    -13.107200 \n    0.9830327 \n  \n  \n    2 \n    0.2297645 \n    0.0658398 \n    5.2228864 \n    -13.107200 \n    0.6102878 \n  \n  \n    3 \n    0.2503131 \n    0.0462667 \n    0.1416746 \n    -3.277992 \n    0.5888936 \n  \n  \n    4 \n    0.2491826 \n    0.0724227 \n    -2.5313281 \n    2.066142 \n    0.5743991 \n  \n  \n    5 \n    0.2695487 \n    0.0557993 \n    0.3566254 \n    -3.371428 \n    0.5619755 \n  \n  \n    6 \n    0.2668834 \n    0.0809960 \n    -2.5091373 \n    1.953857 \n    0.5470039 \n  \n  \n    7 \n    0.2881952 \n    0.0644006 \n    0.7270034 \n    -3.731174 \n    0.5414702 \n  \n  \n    8 \n    0.2827931 \n    0.0921256 \n    -2.8092036 \n    2.430734 \n    0.5291569 \n  \n  \n    9 \n    0.3046506 \n    0.0732128 \n    0.9976613 \n    -3.919835 \n    0.5219235 \n  \n  \n    10 \n    0.2981029 \n    0.0989389 \n    -2.6049709 \n    2.014701 \n    0.5028071 \n  \n  \n    11 \n    0.3187362 \n    0.0829810 \n    1.0103687 \n    -3.722352 \n    0.4987602 \n  \n  \n    12 \n    0.3114437 \n    0.1098474 \n    -2.9779538 \n    2.570033 \n    0.4906224 \n  \n  \n    13 \n    0.3321087 \n    0.0920131 \n    1.0930160 \n    -3.656631 \n    0.4795061 \n  \n  \n    14 \n    0.3240513 \n    0.1189688 \n    -3.1613443 \n    2.791914 \n    0.4763936 \n  \n\n\n\n\n\nNewton-Raphson\n\nNR <- function(start, n.iter, alpha, rho) {\n  \n  b <- start\n  grad <- matrix(0, n.iter + 1, 2)\n  grad[1, ] <- score(b[1], b[2])\n  \n  out <- matrix(0, n.iter + 1, 2)\n  out[1, ] <- b\n  \n  for (i in 1:n.iter + 1) {\n    \n    fold <- f(out[i-1,1], out[i-1,2])\n    b <- out[i - 1, ]\n    \n    out[i, ] <- b - solve(hess(b[1], b[2])) %*% score(b[1], b[2])\n    grad[i, ] <- c(score(b[1], b[2]))\n\n    fnew <- f(out[i,1], out[i,2])\n    a <- alpha\n    while (fnew>fold){\n      a <- a*rho\n      out[i,] <- out[i-1,] - a * solve(hess(b[1], b[2])) %*% score(b[1], b[2])\n      grad[i, ] <- c(score(out[i,1], out[i,2]))\n      fnew <- f(out[i,1], out[i,2])\n    }\n  }\n  data.frame(iter = 0:(nrow(out)-1), \n             out  = out, \n             grad = grad, \n             fval = f(out[,1], out[,2]))\n}\nNRout <- NR(c(0,0), 15, 1, 0.8) \n\nNRout |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    out.1 \n    out.2 \n    grad.1 \n    grad.2 \n    fval \n  \n \n\n  \n    0 \n    0.0000000 \n    0.0000000 \n    -2.0000000 \n    0.0000000 \n    1.0000000 \n  \n  \n    1 \n    0.2097152 \n    0.0000000 \n    2.1087792 \n    -8.7960930 \n    0.8179782 \n  \n  \n    2 \n    0.2903887 \n    0.0778174 \n    2.1087792 \n    -8.7960930 \n    0.5077839 \n  \n  \n    3 \n    0.4877049 \n    0.1965794 \n    7.0277391 \n    -8.2553296 \n    0.4328224 \n  \n  \n    4 \n    0.5430563 \n    0.2918463 \n    7.0277391 \n    -8.2553296 \n    0.2097363 \n  \n  \n    5 \n    0.7243882 \n    0.4907541 \n    9.2958913 \n    -6.7968490 \n    0.1914547 \n  \n  \n    6 \n    0.7597374 \n    0.5759513 \n    9.2958913 \n    -6.7968490 \n    0.0578823 \n  \n  \n    7 \n    0.8827605 \n    0.7636815 \n    5.2684843 \n    -3.1169062 \n    0.0380329 \n  \n  \n    8 \n    0.9112381 \n    0.8295438 \n    5.2684843 \n    -3.1169062 \n    0.0079444 \n  \n  \n    9 \n    0.9876125 \n    0.9695454 \n    0.1180717 \n    -0.1621945 \n    0.0035559 \n  \n  \n    10 \n    0.9933299 \n    0.9866717 \n    2.2795435 \n    -1.1666107 \n    0.0000446 \n  \n  \n    11 \n    0.9999567 \n    0.9998694 \n    -0.0003516 \n    -0.0065379 \n    0.0000002 \n  \n  \n    12 \n    0.9999996 \n    0.9999992 \n    0.0174780 \n    -0.0087827 \n    0.0000000 \n  \n  \n    13 \n    1.0000000 \n    1.0000000 \n    0.0000000 \n    -0.0000004 \n    0.0000000 \n  \n  \n    14 \n    1.0000000 \n    1.0000000 \n    0.0000000 \n    0.0000000 \n    0.0000000 \n  \n  \n    15 \n    1.0000000 \n    1.0000000 \n    0.0000000 \n    0.0000000 \n    0.0000000 \n  \n\n\n\n\nexpand.grid(x1 = -50:150/100,\n            x2 = -50:150/100) |>\n  mutate(f = map2_dbl(x1, x2, fx1x2)) |>\n  ggplot(aes(x = x1, y = x2, z = f)) +\n  stat_contour_filled(bins = 50, show.legend = FALSE) +\n  geom_line(data = SDout, \n            mapping = aes(x = out.1, y = out.2, z = NULL), \n            col = \"yellow\") +\n  geom_line(data = NRout, \n            mapping = aes(x = out.1, y = out.2, z = NULL),\n            col = \"orange\") +\n  theme_minimal() +\n  labs(x = expression(italic(X[1])),\n       y = expression(italic(X[2])),\n       title = \"Contour plot\")\n\n\n\n\n4. Suppose for an individual during consecutive nights, it is recorded how loudly he snores (covariate \\(x\\)) and whether he wakes up or not (the outcome \\(Y\\)). Consider the following hypothetical data are collected: \\(\\boldsymbol{x} = (0,1,2,3,4,5)'\\) and \\(\\boldsymbol{y} = (0,1,0,1,1,1)'\\). A logistic regression model is put forward for these data such that \\(\\text{logit}(\\Pr(y_i = 1 | x_i)) = \\text{logit}(\\pi(x_i)) = \\beta_0 + \\beta_1 x_i\\). Starting from \\(\\boldsymbol{\\beta}^{(0)} = (0,0)'\\), perform the first five steps of the Newton-Raphson algorithm to find the maximum of the likelihood. Put your results in a table with as columns: iteration number, current point and loglikelihood value. Do the same for iterative reweighted least squares.\nSolution\nFor logistic regression, the likelihood is defined as\n\\[\n\\begin{aligned}\nL &= \\prod^N_{i=1} \\frac{e^{y_i(\\beta_0 + \\beta_1 x_i)}}{1 + e^{(\\beta_0 + \\beta_1x_i)}}, \\\\\n\\ell &= \\sum^N_{i=1} y_i(\\beta_0 + \\beta_1x_i) - \\log(1 + e^{(\\beta_0 + \\beta_1x_i)}).\n\\end{aligned}\n\\]\nAdditionally, the Gradient is defined by\n\\[\n\\nabla \\ell(\\beta_0, \\beta_1) = \\begin{pmatrix}\n\\sum^N_{i=1} y_i - \\frac{e^{(\\beta_0 + \\beta_1 x_i)}}{1 + e^{(\\beta_0 + \\beta_1 x_i)}} \\\\\n\\sum^N_{i=1}x_i (y_i - \\frac{e^{(\\beta_0 + \\beta_1 x_i)}}{1 + e^{(\\beta_0 + \\beta_1 x_i)}})\n\\end{pmatrix},\n\\]\nwhile the Hessian is defined as\n\\[\n\\nabla^2 \\ell(\\beta_0, \\beta_1) = \\begin{pmatrix}\n- \\sum^N_{i=1} \\frac{e^{(\\beta_0 + \\beta_1 x_i)}}{(1 + e^{(\\beta_0 + \\beta_1 x_i)})^2} &\n- \\sum^N_{i=1}x_i \\frac{e^{(\\beta_0 + \\beta_1 x_i)}}{(1 + e^{(\\beta_0 + \\beta_1 x_i)})^2} \\\\\n- \\sum^N_{i=1}x_i \\frac{e^{(\\beta_0 + \\beta_1 x_i)}}{(1 + e^{(\\beta_0 + \\beta_1 x_i)})^2} &\n- \\sum^N_{i=1} x_i^2\\frac{e^{(\\beta_0 + \\beta_1 x_i)}}{(1 + e^{(\\beta_0 + \\beta_1 x_i)})^2}\n\\end{pmatrix}.\n\\]\n\nloglikelihood <- function(X, Y, beta) {\n  sum(Y * (X%*%beta) - log(1 + exp(X %*% beta)))\n}\nscore <- function(X, Y, beta) {\n  t(X) %*% (Y - 1/(1 + exp(-X%*%beta)))\n}\nhess <- function(X, Y, beta) {\n  - t(X) %*% diag(c(exp(X%*%beta)/(1 + exp(X%*%beta))^2)) %*% X\n}\n\nNewton-Raphson implementation\n\nNRlogistic <- function(formula, data = NULL, start, n.iter) {\n  X <- model.matrix(formula, data)\n  Y <- model.frame(formula, data)[, 1]\n  \n  out <- matrix(0, n.iter+1, ncol(X))\n  out[1, ] <- b <- start\n  \n  logL <- numeric(n.iter+1)\n  logL <- loglikelihood(X, Y, b)\n  \n  for (i in 1:n.iter + 1) {\n    b <- b - solve(hess(X, Y, b)) %*% score(X, Y, b)\n    out[i, ] <- b\n    logL[i] <- loglikelihood(X, Y, b)\n  }\n  \n  data.frame(iter = 0:n.iter,\n             b0 = out[,1],\n             b1 = out[,2],\n             logL = logL)\n}\n\nx <- c(0,1,2,3,4,5)\ny <- c(0,1,0,1,1,1)\n\nNRlogistic(y ~ x, start = c(0,0), n.iter = 5) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    b0 \n    b1 \n    logL \n  \n \n\n  \n    0 \n    0.000000 \n    0.0000000 \n    -4.158883 \n  \n  \n    1 \n    -1.047619 \n    0.6857143 \n    -2.626827 \n  \n  \n    2 \n    -1.444172 \n    0.9933894 \n    -2.457094 \n  \n  \n    3 \n    -1.602433 \n    1.1249532 \n    -2.440395 \n  \n  \n    4 \n    -1.624928 \n    1.1443026 \n    -2.440125 \n  \n  \n    5 \n    -1.625338 \n    1.1446616 \n    -2.440125 \n  \n\n\n\n\nglm(y ~ x, family = binomial) |> summary()\n\n\nCall:\nglm(formula = y ~ x, family = binomial)\n\nDeviance Residuals: \n      1        2        3        4        5        6  \n-0.5995   1.3872  -1.4692   0.5509   0.3189   0.1815  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -1.6253     1.9284  -0.843    0.399\nx             1.1447     0.9278   1.234    0.217\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 7.6382  on 5  degrees of freedom\nResidual deviance: 4.8802  on 4  degrees of freedom\nAIC: 8.8802\n\nNumber of Fisher Scoring iterations: 5\n\n\nConvergence is reached after five iterations!\nIterative re-weighted least squares implementation\n\nIRLS <- function(formula, data = NULL, start, n.iter) {\n  \n  X <- model.matrix(formula, data)\n  Y <- model.frame(formula, data)[,1]\n  out <- matrix(0, n.iter+1, ncol(X))\n  out[1, ] <- b <- start\n  \n  logL <- numeric(n.iter+1)\n  logL[1] <- loglikelihood(X, Y, b)\n  \n  for (i in 1:n.iter+1) {\n    e <- exp(X%*%b) / (1 + exp(X%*%b))\n    W <- diag(c(e / (1+exp(X %*% b))))\n    Z <- X %*% b + (y - e) * (1 / (e*(1-e)))\n    b <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Z\n    out[i, ] <- b\n    logL[i] <- loglikelihood(X, Y, b)\n  }\n  data.frame(iter = 0:n.iter,\n             b0 = out[,1],\n             b1 = out[,2],\n             logL = logL)\n}\nIRLS(y ~ x, start = c(0,0), n.iter = 5) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    b0 \n    b1 \n    logL \n  \n \n\n  \n    0 \n    0.000000 \n    0.0000000 \n    -4.158883 \n  \n  \n    1 \n    -1.047619 \n    0.6857143 \n    -2.626827 \n  \n  \n    2 \n    -1.444172 \n    0.9933894 \n    -2.457094 \n  \n  \n    3 \n    -1.602433 \n    1.1249532 \n    -2.440395 \n  \n  \n    4 \n    -1.624928 \n    1.1443026 \n    -2.440125 \n  \n  \n    5 \n    -1.625338 \n    1.1446616 \n    -2.440125 \n  \n\n\n\n\n\nAnd again, convergence is reached after five iterations!\n5. Consider the function\n\\[\nf(x) = \\frac{e^x}{(1 + e^x)^2}.\n\\]\nUsing an iterative procedure of your liking, find the optimum of the function, and check whether it is a minimum or a maximum.\nSolution\nFirst, calculate we calculate the derivatives.\n\\[\n\\begin{aligned}\nf(x) &= \\frac{e^x}{(1 + e^x)^2}, \\\\\nf'(x) &= \\frac{e^x - e^{2x}}{(1+e^x)^3}, \\\\\nf''(x) &= \\frac{e^x - 4e^{2x} + e^{3x}}{(1+e^x)^4}.\n\\end{aligned}\n\\]\nWe can first find the optimum analytically. Let’s first take the log of the function, which makes it easier to work with:\n\\[\n\\log f(x) = x - 2 \\log (1+e^x).\n\\]\nSubsequently, we take the derivative of \\(\\log f(x)\\) and set it equal to zero to find the optimum.\n\\[\n\\begin{aligned}\n\\frac{\\partial f}{\\partial x} = 1 - \\frac{2e^x}{1+e^x} &= 0, \\\\\n\\implies \\frac{2e^x}{1+e^x} &= 1, \\\\\n1+e^x &= 2e^x, \\\\\ne^x &= 1, \\\\\nx &= 0.\n\\end{aligned}\n\\]\nSo we know the solution must be \\(x=0\\). Doing the same steps using the Newton-Raphson algorithm yields\n\nfx <- function(x) -exp(x) / (1 + exp(x))^2\nf1x <- function(x) -(exp(x) - exp(2*x)) / (1 + exp(x))^3\nf2x <- function(x) -(exp(x) - 4*exp(2*x) + exp(3*x)) / (1 + exp(x))^4\n\nNR <- function(start = 0.5, n.iter = 20, alpha = 1, rho = 0.8) {\n  out <- matrix(0, n.iter+1, 4)\n  out[1, ] <- c(start, fx(start), f1x(start), f2x(start))\n  \n  colnames(out) <- c(\"x\", \"fx\", \"f1x\", \"f2x\")\n  \n  for (i in 1:n.iter + 1) {\n    a <- alpha\n    new <- out[i-1, 1] - a * out[i-1, 3] / out[i-1, 4]\n    out[i, ] <- c(new, fx(new), f1x(new), f2x(new))\n    while(out[i, 2] > out[i-1, 2]) {\n      a <- a*rho\n      new <- out[i-1, 1] - a * out[i-1, 3] / out[i-1, 4]\n      out[i, ] <- c(new, fx(new), f1x(new), f2x(new))\n    }\n  }\n  out\n}\n\nNR(0.5, 5) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    x \n    fx \n    f1x \n    f2x \n  \n \n\n  \n    0.5000000 \n    -0.2350037 \n    0.0575568 \n    0.0963568 \n  \n  \n    -0.0973301 \n    -0.2494089 \n    -0.0121279 \n    0.1238198 \n  \n  \n    0.0006180 \n    -0.2500000 \n    0.0000773 \n    0.1250000 \n  \n  \n    0.0000000 \n    -0.2500000 \n    0.0000000 \n    0.1250000 \n  \n  \n    0.0000000 \n    -0.2500000 \n    0.0000000 \n    0.1250000 \n  \n  \n    0.0000000 \n    -0.2500000 \n    0.0000000 \n    0.1250000 \n  \n\n\n\n\n\n6. Continuation of exercise 6 from chapter 3: implement maximum likelihood estimation for this logistic regression.\nSolution\n\nx <- c(0.5, 1, 1.5, 2, 2.5)\ny <- c(0,0,1,0,1)\n\nNRlogistic(y ~ x, start = c(0,0), n.iter = 5) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    b0 \n    b1 \n    logL \n  \n \n\n  \n    0 \n    0.000000 \n    0.000000 \n    -3.465736 \n  \n  \n    1 \n    -2.800000 \n    1.600000 \n    -2.479523 \n  \n  \n    2 \n    -3.698907 \n    2.079500 \n    -2.423599 \n  \n  \n    3 \n    -3.886773 \n    2.177155 \n    -2.421969 \n  \n  \n    4 \n    -3.893957 \n    2.180846 \n    -2.421967 \n  \n  \n    5 \n    -3.893967 \n    2.180851 \n    -2.421967 \n  \n\n\n\n\nIRLS(y ~ x, start = c(0,0), n.iter = 5) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    iter \n    b0 \n    b1 \n    logL \n  \n \n\n  \n    0 \n    0.000000 \n    0.000000 \n    -3.465736 \n  \n  \n    1 \n    -2.800000 \n    1.600000 \n    -2.479523 \n  \n  \n    2 \n    -3.698907 \n    2.079500 \n    -2.423599 \n  \n  \n    3 \n    -3.886773 \n    2.177155 \n    -2.421969 \n  \n  \n    4 \n    -3.893957 \n    2.180846 \n    -2.421967 \n  \n  \n    5 \n    -3.893967 \n    2.180851 \n    -2.421967 \n  \n\n\n\n\nglm(y ~ x, family = binomial) |> summary()\n\n\nCall:\nglm(formula = y ~ x, family = binomial)\n\nDeviance Residuals: \n      1        2        3        4        5  \n-0.3430  -0.5758   1.4506  -1.3814   0.6181  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -3.894      3.465  -1.124    0.261\nx              2.181      1.950   1.119    0.263\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.7301  on 4  degrees of freedom\nResidual deviance: 4.8439  on 3  degrees of freedom\nAIC: 8.8439\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "chapter7.html",
    "href": "chapter7.html",
    "title": "7  The MM algorithm with applications to regularized regression",
    "section": "",
    "text": "Example: A majorizing algorithm for the median\n\nMM_median <- function(x, start, maxit, tol) {\n  theta <- numeric(length = maxit)\n  theta[1] <- start\n  t <- 1\n  conv <- FALSE\n  \n  while (!conv) {\n    t <- t+1\n    theta[t] <- 1/(sum(1/abs(x-theta[t-1]))) * sum(x / abs(x - theta[t-1]))\n    \n    if (t == maxit | abs(theta[t] - theta[t-1]) < tol) {\n      conv <- TRUE\n    }\n  }\n  theta[1:t]\n}\n\nx <- c(1,0,9,5,1)\nMM_median(x, mean(x), 50, 1e-10)\n\n [1] 3.200000 2.687064 2.221601 1.836210 1.541400 1.331585 1.192418 1.106385\n [9] 1.056601 1.029324 1.014946 1.007548 1.003794 1.001902 1.000952 1.000476\n[17] 1.000238 1.000119 1.000060 1.000030 1.000015 1.000007 1.000004 1.000002\n[25] 1.000001 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000\n[33] 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000 1.000000\n\nmedian(x)\n\n[1] 1\n\n\nWe obtained the same solution.\nExample: Multiple linear lasso regression\n\nset.seed(1)\n\nN <- 10000\nP <- 100\nB <- c(1:10/5, rep(0, 90))\n\nS <- matrix(0.5, P, P)\ndiag(S) <- 1\nX <- matrix(rnorm(N*P), N, P) %*% chol(S)\nY <- X %*% B + rnorm(N, 0, 10)\n\nX <- scale(X)\nY <- scale(Y)\n\ncv_lasso <- glmnet::cv.glmnet(X, Y, alpha = 1, standardize = F,\n                              intercept = FALSE)\nlasso <- glmnet::glmnet(X, Y, alpha = 1, standardize = F, \n                        intercept = F,\n                        lambda = cv_lasso$lambda.min)\n\n\nMM_lasso <- function(X, Y, start, lambda, threshold, maxit, tol) {\n\n  b <- matrix(0, maxit, ncol(X))\n  b[1, ] <- start\n  \n  t <- 1\n  conv <- FALSE\n  \n  while(!conv) {\n    t <- t+1\n    D <- diag(1/(abs(b[t-1,]) + threshold))\n    b[t, ] <- solve(t(X) %*% X + lambda/2 * D) %*% t(X) %*% Y\n    if (t == maxit | sum(abs(b[t] - b[t-1])) < tol) conv <- TRUE\n  }\n  round(b[1:t,],5)\n}\n\nown_lasso <- MM_lasso(X = X, \n                      Y = Y, \n                      start = runif(rep(1, P)), \n                      lambda = 2*N*cv_lasso$lambda.min, \n                      threshold = 1e-7,\n                      maxit = 500, \n                      tol = 1e-14)\n\ndplyr::bind_cols(glmnet = lasso$beta[1:15,],\n                 MM_lasso = own_lasso[nrow(own_lasso), 1:15, drop=F] |> c()) |>\n  knitr::kable() |>\n  kableExtra::kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\n \n  \n    glmnet \n    MM_lasso \n  \n \n\n  \n    0.0023482 \n    0.00202 \n  \n  \n    0.0136856 \n    0.01325 \n  \n  \n    0.0226324 \n    0.02220 \n  \n  \n    0.0403210 \n    0.03987 \n  \n  \n    0.0572259 \n    0.05680 \n  \n  \n    0.0715446 \n    0.07115 \n  \n  \n    0.1105939 \n    0.11025 \n  \n  \n    0.1172435 \n    0.11695 \n  \n  \n    0.1465717 \n    0.14635 \n  \n  \n    0.1235894 \n    0.12347 \n  \n  \n    0.0076866 \n    0.00769 \n  \n  \n    0.0000000 \n    0.00000 \n  \n  \n    0.0000000 \n    0.00000 \n  \n  \n    0.0000000 \n    0.00000 \n  \n  \n    0.0000000 \n    0.00000 \n  \n\n\n\n\n\nThe results are not the same, but they do come quite close.\n\nn <- 100; p <- 1000\ns <- matrix(0.95, p, p)\ndiag(s) <- 1\n\nx <- matrix(rnorm(n*p), n, p) %*% chol(s)\nb <- runif(p)\ny <- x %*% b + rnorm(n, 0, 500)\n\nout <- MM_lasso(x, y, runif(p), 10000, .000001, 500, 1e-15)\nout[nrow(out), ][abs(out[nrow(out), ]) > 0.00001]\n\n [1]  10.84720   4.01224 106.27432   0.00004  71.19764  40.71499   0.00019\n [8]  72.95655   8.50397   0.00003   0.00003  68.22038   0.00026   7.40789\n[15]  76.84439   0.00002   0.00004   0.00003   0.00002   0.00002\n\n\n\n8 Exercises\n1. Write a script to generate data with the following structure: \\(p = 9\\) predictors with the following covariance structure \\[\n\\boldsymbol{R} = \\begin{pmatrix}\n1 & 0.8 & 0.7 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0.8 & 1 & 0.6 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0.7 & 0.6 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & 1 & 0.2 & 0.4 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & 0.2 & 1 & 0.3 \\\\\n0   & 0   & 0 & 0 & 0 & 0 & 0.4 & 0.3 & 1\n\\end{pmatrix}\n\\] __and \\(y_i = \\boldsymbol{x}_i'\\boldsymbol{\\beta} + e_i\\) with \\(\\boldsymbol{\\beta} = [0,1,2,0,0,3,1.5,1.5,0]'\\) and \\(e_i \\sim \\mathcal{N}(0,10)\\).\nSolution\n\nset.seed(123)\nN <- 1000\nP <- 9\nR <- diag(P)\nR[1:3, 1:3] <- c(1, 0.8, 0.7, 0.8, 1, 0.6, 0.7, 0.6, 1)\nR[7:9, 7:9] <- c(1, 0.2, 0.4, 0.2, 1, 0.4, 0.4, 0.3, 1)\n# X <- matrix(rnorm(N*P), N, P) %*% chol(R)\nB <- c(0, 1, 2, 0, 0, 3, 1.5, 1.5, 0)\n# Y <- X %*% B + rnorm(N, 0, 10)\n\nXrand <- matrix(rnorm(N*P), N, P)\nXc <- Xrand - rep(1, N) %*% t(colMeans(Xrand))\nS <- svd(Xc)\nnewX <- sqrt(N-1) * S$u %*% chol(R)\nY <- newX %*% B + 10*rnorm(N)\n\n2. Implement the MM algorithm for the elastic net.\n\nlibrary(ggplot2)\n\nMM_elastic_net <- function(X, Y, start, alpha, eps, l1, l2, maxit, tol) {\n  b <- matrix(0, maxit, ncol(X))\n  b[1, ] <- start\n  t <- 1\n  conv <- FALSE\n  \n  purrr::map2(l1, l2, function(L1, L2) {\n    while(!conv) {\n      t <- t + 1\n      D <- solve(diag(c(abs(b[t-1, ]) + eps)))\n      b[t, ] <- solve(t(X) %*% X + alpha * L1/2 * D + (1-alpha) * L2 * diag(ncol(X))) %*% t(X) %*% Y\n      b[t, which(abs(b[t, ]) < 1e-5)] <- 0\n      \n      if (sum(abs(b[t, ] - b[t-1])) < tol | maxit == t) {\n        conv <- T\n      }\n    }\n    b[t, ]\n  })\n}\n\nL <- expand.grid(L1 = c(20, 15, 11, 10, 8, 6, 5, 4, 3, 2, 1, 0.9, 0.8, 0.7, 0.65, \n                        0.6, 0.5, 0.4, 0.35, 0.25, 0.2, 0.15, 0.125, 0.10, 0.09, \n                        0.08, 0.05, 0.02, 0.015, 0.001, 0.0001, 0.00001, 0) * 2*N,\n                 L2 = c(0, 0.0001, 0.5, 1) * 2*N)\nout <- MM_elastic_net(newX, Y, runif(P), alpha = 0.5, eps = 0.001, \n                      l1 = L$L1, l2 = L$L2, maxit = 1000, tol = 1e-14)\n\nresults <- tibble::tibble(\n  L1 = L$L1,\n  L2 = L$L2,\n  B = out\n) |>\n  tidyr::unnest(B) |>\n  dplyr::mutate(Beta = rep(paste0(1:P), length(out)))\n\nresults |>\n  dplyr::mutate(L1 = factor(L1 / N / 2),\n                L2 = factor(L2 / N / 2)) |>\n  ggplot(aes(x = L1, y = B, col = Beta, group = Beta)) +\n  geom_line() +\n  facet_wrap(~L2) +\n  theme_minimal() +\n  scale_color_viridis_d(name = expression(beta)) +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6)) +\n  labs(y = expression(beta))"
  },
  {
    "objectID": "chapter8.html",
    "href": "chapter8.html",
    "title": "8  Constrained optimization",
    "section": "",
    "text": "To be added."
  },
  {
    "objectID": "chapter9.html",
    "href": "chapter9.html",
    "title": "9  Maximum Likelihood Estimation and Inference",
    "section": "",
    "text": "\\[\n\\begin{pmatrix} \\hat{\\alpha} \\\\ \\hat{\\beta} \\end{pmatrix} =\n\\begin{pmatrix} - 3.89 \\\\ 2.18 \\end{pmatrix}.\n\\]\nObtain the asymptotic variance-covariance matrix and standard errors for \\(\\hat{\\alpha}\\) and \\(\\hat{\\beta}\\), and implement a test for \\(H_0: \\alpha = 1\\) and \\(\\beta = 0\\).\nSolution\nRecall that the first- and second-order derivatives are defined as\n\\[\n\\begin{aligned}\n\\nabla \\ell = S(\\alpha, \\beta) &=\n\\begin{pmatrix}\n\\sum^n_{i=1} y_i - \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}} \\\\\n\\sum^n_{i=1} x_i(y_i - \\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}})\n\\end{pmatrix} \\\\\n&= X' \\Bigg(Y - \\frac{e^{X\\beta}}{1+e^{X\\beta}}\\Bigg) \\\\\n\\nabla^2 \\ell = H(\\alpha, \\beta) &=\n\\begin{pmatrix}\n- \\sum^n_{i=1} \\frac{e^{\\alpha + \\beta x_i}}{(1 + e^{\\alpha + \\beta x_i})^2} &\n- \\sum^n_{i=1} x_i(\\frac{e^{\\alpha + \\beta x_i}}{(1 + e^{\\alpha + \\beta x_i})^2}) \\\\\n- \\sum^n_{i=1} x_i(\\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}) &\n- \\sum^n_{i=1} x_i^2(\\frac{e^{\\alpha + \\beta x_i}}{1 + e^{\\alpha + \\beta x_i}}) \\\\\n\\end{pmatrix} \\\\\n&= X' \\text{diag}\\Bigg(\\frac{e^{X \\beta}}{1+e^{X \\beta}}\\Bigg) X.\n\\end{aligned}\n\\]\nThe standard errors are defined as \\(\\sqrt{\\text{diag}[-H(\\alpha, \\beta)]'}\\). So, we have\n\nloglikelihood <- function(Y, X, beta) {\n  sum(Y*(X %*% beta) - log(1 + exp(X %*% beta)))\n}\nscore <- function(Y, X, beta) {\n  t(X) %*% (Y - 1 / (1 + exp(-X %*% beta)))\n}\nhessian <- function(X, beta) {\n  - t(X) %*% diag(c(exp(X %*% beta) / (1 + exp(X %*% beta))^2)) %*% X\n}\n\nNR <- function(formula, data = NULL, n.iter) {\n  X <- model.matrix(formula, data)\n  Y <- model.frame(formula, data)[,1]\n  \n  beta <- matrix(0, n.iter + 1, ncol(X))\n  \n  L <- numeric(n.iter)\n  L[1] <- loglikelihood(Y, X, beta[1, ])\n  t <- 1; conv <- FALSE\n  \n  while(!conv) {\n    t <- t + 1\n    beta[t, ] <- beta[t-1, ] - solve(hessian(X, beta[t-1, ])) %*% score(Y, X, beta[t-1, ])\n    L[t] <- loglikelihood(Y, X, beta[t, ])\n    \n    if (abs(L[t] - L[t-1]) < 1e-10 | t == n.iter) conv <- TRUE\n  }\n  \n  list(b = beta[1:t, ], \n       se = sqrt(diag(solve(-hessian(X, beta[t, ])))),\n       loglik = L[1:t])\n}\n\nx <- c(0.5,1,1.5,2,2.5)\ny <- c(0,0,1,0,1)\n\nout <- NR(y ~ x, n.iter = 50)\nout\n\n$b\n          [,1]     [,2]\n[1,]  0.000000 0.000000\n[2,] -2.800000 1.600000\n[3,] -3.698907 2.079500\n[4,] -3.886773 2.177155\n[5,] -3.893957 2.180846\n[6,] -3.893967 2.180851\n\n$se\n(Intercept)           x \n   3.465687    1.949705 \n\n$loglik\n[1] -3.465736 -2.479523 -2.423599 -2.421969 -2.421967 -2.421967\n\nb <- out$b[nrow(out$b),]\nsummary(glm(y ~ x, family = binomial))\n\n\nCall:\nglm(formula = y ~ x, family = binomial)\n\nDeviance Residuals: \n      1        2        3        4        5  \n-0.3430  -0.5758   1.4506  -1.3814   0.6181  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -3.894      3.465  -1.124    0.261\nx              2.181      1.950   1.119    0.263\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 6.7301  on 4  degrees of freedom\nResidual deviance: 4.8439  on 3  degrees of freedom\nAIC: 8.8439\n\nNumber of Fisher Scoring iterations: 4\n\nLT <- b - c(1, 0)\nLHL <- t(diag(2)) %*% (-solve(hessian(cbind(1,x), b))) %*% diag(2)\n\npchisq(t(LT) %*% solve(LHL) %*% LT, 2, lower.tail = FALSE)\n\n          [,1]\n[1,] 0.2949003\n\n\n2. Assume that you maximize \\(\\ell(\\alpha; p, n) = p\\alpha - n \\ln (1 + e^\\alpha)\\) to estimate \\(\\alpha\\). In a next step, you want to find the optimal \\(\\pi = \\frac{e^\\alpha}{1+e^{\\alpha}}\\). What is the MLE of \\(\\pi\\), and what is the standard error of \\(\\pi\\) (use the delta method).\nSolution\n\\[\n\\begin{aligned}\n\\ell(\\alpha; p, n) &= p\\alpha - n \\ln (1 + e^\\alpha) \\\\\n\\frac{\\partial \\ell}{\\partial \\alpha} &=\np - n \\frac{e^\\alpha}{1 + e^\\alpha} \\\\\n\\implies 0 &= p - n\\pi \\\\\n\\implies \\pi &= \\frac{p}{n}\n\\end{aligned}\n\\]\nUsing the delta method to obtain the standard error yields\n\\[\n\\begin{aligned}\n\\text{Var}(F(\\hat\\theta)) &= \\frac{\\partial F(\\hat\\theta)}{\\partial \\theta'} (-\\hat{H})^{-1} \\frac{\\partial F(\\hat\\theta)}{\\partial \\theta} \\\\\n&= \\frac{e^a}{(1+e^a)^2} \\frac{(1+e^a)^2}{n e^a} \\frac{e^a}{(1+e^a)^2} \\\\\n&= \\pi(1-\\pi) \\frac{1}{n\\pi(1-\\pi)} \\pi(1-\\pi) \\\\\n&= \\frac{\\pi(1-\\pi)}{n}, \\\\\n\\text{SE}(F(\\hat\\theta)) &= \\sqrt{\\text{Var}(F(\\hat\\theta))} \\\\\n&= \\sqrt{\\frac{\\pi(1-\\pi)}{n}}.\n\\end{aligned}\n\\]\n3. Suppose the maximum likelihood estimate of a probability \\(\\pi\\) is \\(\\hat\\pi = 0.75\\) (based on \\(n = 10\\) observations). The (approximate) standard error of this estimate is \\(SE(\\hat\\pi) \\approx \\sqrt{\\frac{\\hat\\pi(1-\\hat\\pi)}{10}}\\). What is the MLE of \\(\\log \\pi\\) and what is the corresponding standard error.\nSolution\nThe MLE of \\(\\log \\pi\\) equals\n\\[\n\\begin{aligned}\n\\text{MLE}(\\log \\pi) &= \\log \\text{MLE} (\\pi) \\\\\n&= \\log 0.75 \\\\\n&\\approx -0.2877.\n\\end{aligned}\n\\]\nAdditionally, the standard error equals\n\\[\n\\begin{aligned}\n\\text{SE}(F(\\hat\\theta)) &= \\sqrt{\\text{Var}(F(\\hat\\theta))} \\\\\n&\\approx \\sqrt{\\frac{\\partial F(\\hat\\theta)}{\\partial \\theta'} (-\\hat{H})^{-1} \\frac{\\partial F(\\hat\\theta)}{\\partial \\theta}} \\\\\n&= \\sqrt{\\Bigg(1 - \\frac{e^a}{1+e^a}\\Bigg) \\frac{n e^a}{(1 + e^a)^2} \\Bigg(1 - \\frac{e^a}{1+e^a}\\Bigg)} \\\\\n&= \\sqrt{(1-\\hat\\pi)\\frac{1}{n\\hat\\pi(1-\\hat\\pi)}(1-\\hat\\pi)} \\\\\n&= \\sqrt{\\frac{1-\\hat\\pi}{n\\hat\\pi}} \\\\\n&= \\sqrt{\\frac{0.25}{7.5}} \\approx 0.1826.\n\\end{aligned}\n\\]"
  }
]