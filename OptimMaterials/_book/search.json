[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Optimization and Numerical Methods Solutions",
    "section": "",
    "text": "This project has two purposes. First, it is an attempt to organize my solutions to the course Optimization and Numerical Methods in a structured way. Second, it provides a justification to try and learn Quarto."
  },
  {
    "objectID": "chapter1.html",
    "href": "chapter1.html",
    "title": "1  Chapter 1",
    "section": "",
    "text": "No exercises."
  },
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "2  Chapter 2",
    "section": "",
    "text": "Chapter 2 is the first chapter that actually entails exercises."
  },
  {
    "objectID": "chapter2.html#exercises-2.7-in-the-notes",
    "href": "chapter2.html#exercises-2.7-in-the-notes",
    "title": "2  Chapter 2",
    "section": "2.1 Exercises (2.7 in the notes)",
    "text": "2.1 Exercises (2.7 in the notes)\n1. Consider the multinomial likelihood in Equation 2.1 for a model (for a two-way contingency table) assuming independence. Can you simplify the likelihood?\n\\[\n\\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk})~~~~~~~~~~~~~~~~ \\sum_{j=1}^R \\sum_{k=1}^C \\pi_{jk}=1\n\\tag{2.1}\\]\nSolution\n\\[\n\\begin{aligned}\n\\ell(\\pi) &= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{j+} \\cdot \\pi_{+k}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln \\pi_{j+} + n_{jk} \\ln \\pi_{+k} \\\\\n&= \\sum_{j=1}^R n_{j+} \\ln \\pi_{j+} + \\sum_{k=1}^C n_{+k} \\ln \\pi_{+k}\n\\end{aligned}\n\\tag{2.2}\\]\n2. In a mixed model, optimization is carried out using the marginal likelihood (the likelihood with the random effects integrated out). Define the marginal likelihood for the one-way random effects ANOVA model.\nOne-way random effects ANOVA with group-specific effects \\(\\mu_j \\sim \\mathcal{N}(0, \\sigma^2_{\\mu})\\), and\n\\[\ny_{ij} = \\beta + \\mu_j + \\epsilon_{ij},\n\\]\nwith \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2_\\epsilon)\\), with \\(a\\) groups indexed \\(j\\), and \\(n_j\\) individuals in every group.\nSolution\nSo, the likelihood consists of two components. For the individuals within each group, we have\n\\[\n\\prod^{n_j}_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg),\n\\]\nwhereas for the groups themselves, we have\n\\[\n\\prod^{a}_{j=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg).\n\\]\nCombining these components, and integrating out the random effects, we obtain the marginal likelihood\n\\[\n\\prod^{a}_{j=1}\n\\int\n\\prod^{n_j}_{i=1}\n\\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg)\n\\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg)\nd\\mu_j.\n\\]\n3. Suppose you do a simple linear regression analysis using a \\(t_\\nu\\)-distribution for the residuals (density: \\(f_\\nu(y) = C \\sqrt{\\lambda} \\Big(1 + \\frac{\\lambda(y-\\mu)^2}{\\nu}\\Big)^{-\\frac{\\nu+1}{2}}\\) where \\(\\mu\\) is the mean (for \\(\\nu > 1\\)), \\(\\lambda\\) is a scale parameter and \\(C\\) is a normalizing constraint that does not depend on \\(\\mu\\) or \\(\\lambda\\)). Define the (log-)likelihood for \\(n\\) observations \\((y_i, x_i)\\), such that \\(\\mu_i = \\beta_0 + \\beta_1x_i\\).\nSolution\n\\[\n\\begin{aligned}\nL(\\beta) &= \\prod_{i=1}^n C\\sqrt{\\lambda} \\Bigg(1 + \\frac{\\lambda (y_i-\\beta_0-\\beta_1x_i)^2}{\\nu}  \\Bigg)^{-\\frac{\\nu+1}{2}}, \\\\\n\\ell(\\beta) &= N \\ln C +  \\frac{N}{2} \\ln \\lambda - \\sum^n_{i=1} \\frac{\\nu + 1}{2} \\ln \\Bigg(1 + \\frac{\\lambda(y_i - \\beta_0 - \\beta_1x_i)^2}{\\nu}\\Bigg)\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "chapter3.html",
    "href": "chapter3.html",
    "title": "3  Basic tools",
    "section": "",
    "text": "Chapter 3 introduces basic tools for optimization problems, such as Taylor Series Expansion, and introduces the exponential family."
  },
  {
    "objectID": "chapter3.html#exercises-3.7-in-the-book",
    "href": "chapter3.html#exercises-3.7-in-the-book",
    "title": "3  Basic tools",
    "section": "3.1 Exercises (3.7 in the book)",
    "text": "3.1 Exercises (3.7 in the book)\n1. Consider \\(f(x) = \\frac{e^x}{1 + e^x}\\). Derive the third-order Taylor series expansion of this function at \\(x = 0\\), and make a graph with the function and the third-order Taylor series expansion at \\(x = 0\\).\nSolution\n\\[\n\\begin{aligned}\nf(x) &= \\frac{e^x}{1+e^x} \\\\\nf'(x) &= \\frac{e^x(1+e^x)}{(1+e^x)^2} - \\frac{e^{2x}}{(1+e^x)^2} = \\frac{e^x}{(1+e^x)^2} \\\\\nf''(x) &= \\frac{e^x(1+e^x)^2 - e^x 2(1 + e^x)e^x}{(1 + e^x)^4} \\\\\n&= \\frac{e^x(1+e^x)^2 - 2e^{2x}}{(1+e^x)^3} \\\\\n&= \\frac{e^x - e^{ 2x}}{(1+e^x)^3} \\\\\nf'''(x) &= \\frac{(e^x-2e^{2x})(1+e^x)^3 - (e^x - e^{2x}) 3 (1+e^x)^2e^x}{(1+e^x)^6} \\\\\n&= \\frac{e^x - 2e^{2x} + e^{2x} - 2e^{3x} - 3e^{2x} + 3e^{3x}}{(1+e^x)^4} \\\\\n&= \\frac{e^x - 4e^{2x} + e^{3x}}{(1+e^x)^4},\n\\end{aligned}\n\\]\nusing Taylor’s theorem, we get\n\\[\n\\begin{aligned}\nf(x) & \\approx \\sum^n_{k=0} \\frac{f^{(k)}(x_0)}{k!} (x - x_0)^k \\\\\n&= \\frac{1}{2} + \\frac{1}{4}x + 0 - \\frac{1}{48}x^3.\n\\end{aligned}\n\\]\n\nlibrary(ggplot2)\nfx <- function(x) exp(x) / (1 + exp(x))\nfx1 <- function(x) exp(x) / (1 + exp(x))^2\nfx2 <- function(x) (exp(x) - exp(2*x)) / (1 + exp(x))^3\nfx3 <- function(x) (exp(x) - 4*exp(2*x) + exp(3*x)) / (1 + exp(x))^4\n\ntaylor <- function(x, root) {\n  fx(root) + fx1(root) * (x - root) + fx2(root) / 2 * (x - root)^2 + fx3(root) / 6 * (x - root)^3\n}\n\nggplot() +\n  geom_function(fun = fx) +\n  geom_function(fun = taylor, args = list(root = 0), col = \"orange\") +\n  xlim(-3, 3) +\n  theme_minimal() +\n  labs(x = \"X\", y = expression(italic(f(X))),\n       title = \"Third-order Taylor Series Expansion\")\n\n\n\n\n2. Consider the function: \\(f(x) = e^{x_1}(4x_1^2 + 2x_2^2 + 4x_1x_2 + 2x_2 + 1)\\). Make a contour plot of this function (let both axes run from -2 to 2) at function values \\(0.2, 0.4, 0.7, 1, 1.7, 1.75, 1.8, 2, 3, 4, 5, 6, 20\\). Derive the second-order Taylor series at \\(\\boldsymbol{x} = (0.5, -1)'\\) and \\(\\boldsymbol{x} = (-0.75, 1)'\\).\nSolution\nContour plot\n\nfx12 <- function(x1, x2) {\n  exp(x1) * (4*x1^2 + 2*x2^2 + 4*x1*x2 + 2*x2 + 1)\n}\n\nexpand.grid(x1 = -200:200/100,\n            x2 = -200:200/100) |>\n  dplyr::mutate(z = fx12(x1, x2)) |>\n  ggplot(aes(x = x1, y = x2, z = z)) +\n  stat_contour_filled(breaks = c(0, 0.2, 0.4, 0.7, 1, 1.7, 1.75, 1.8, 2, 3, 4, 5, 6, 20, 100)) +\n  geom_point(aes(x = 0.5, y = -1), col = \"orange\", shape = \"cross\") +\n  geom_point(aes(x = -1.5, y = 1), col = \"orange\", shape = \"cross\") +\n  theme_minimal() +\n  labs(x = \"X1\", y = \"X2\",\n       title = \"Contour plot\")\n\n\n\n\nThe second-order Taylor expansion uses the first and second partial derivatives of the function \\(f(x)\\).\n\\[\n\\begin{aligned}\nf(x) &= e^{x1}(4e^2_1 + 2x^2_2 + 4x_1x_2 + 2x_2 + 1), \\\\\n\\frac{\\partial f}{\\partial x_1} &= f(x) + e^{x_1}(8x_1 + 4x_2), \\\\\n\\frac{\\partial f}{\\partial x_2} &= e^{x_1} (4x_2 + 4x_1 + 2), \\\\\n\\frac{\\partial^2 f}{\\partial x_1^2} &= f(x) + 2e^{x1}(8x_1 + 4x_2) + 8e^{x_1}, \\\\\n\\frac{\\partial^2 f}{\\partial x_2^2} &= 4e^{x_1}, \\\\\n\\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &= 4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2).\n\\end{aligned}\n\\]\nAccordingly, the Gradient \\(\\nabla f(x)\\) is defined as\n\\[\n\\nabla f(x) =\n\\begin{pmatrix}\nf(x) + e^{x_1}(8x_1 + 4x_2) \\\\\ne^{x_1} (4x_2 + 4x_1 + 2)\n\\end{pmatrix},\n\\]\nand the Hessian \\(\\nabla^2 f(x)\\) is defined as\n\\[\n\\nabla^2 f(x) =\n\\begin{pmatrix}\nf(x) + 2e^{x1}(8x_1 + 4x_2) + 8e^{x_1} &\n4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2) \\\\\n4e^{x_1} + e^{x_1}(4x_2 + 4x_1 + 2) &\n4e^{x_1}\n\\end{pmatrix}.\n\\]\nMoreover, the second-order Taylor series at \\(\\boldsymbol{x} = (0.5, -1)'\\) and \\(\\boldsymbol{x} = (-0.75, 1)'\\) is defined as\n\\[\n\\begin{aligned}\n\\nabla f((0.5, -1)) &=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\\n\\nabla^2 f((0.5, -1)) &=\n\\begin{pmatrix}\n13.19 & 6.59 \\\\\n6.59 & 6.59\n\\end{pmatrix},\n\\end{aligned}\n\\]\nand\n\\[\n\\begin{aligned}\n\\nabla f((-1.5, 1)) &=\n\\begin{pmatrix} 0 \\\\ 0 \\end{pmatrix} \\\\\n\\nabla^2 f((-1.5, 1)) &=\n\\begin{pmatrix}\n0 & 0.89 \\\\\n0.89 & 0.89\n\\end{pmatrix}.\n\\end{aligned}\n\\]\nAs can be seen in the contour plot, the first point is a minimum, while the second point is a saddle point."
  }
]