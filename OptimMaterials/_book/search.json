[
  {
    "objectID": "chapter2.html",
    "href": "chapter2.html",
    "title": "1  Chapter 2",
    "section": "",
    "text": "Chapter 2 is the first chapter that actually entails exercises."
  },
  {
    "objectID": "chapter2.html#section",
    "href": "chapter2.html#section",
    "title": "1  Chapter 2",
    "section": "1.1 2.7",
    "text": "1.1 2.7\n1. Consider the multinomial likelihood in Equation 1.1 for a model (for a two-way contingency table) assuming independence. Can you simplify the likelihood?\n\\[\n\\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk})~~~~~~~~~~~~~~~~ \\sum_{j=1}^R \\sum_{k=1}^C \\pi_{jk}=1\n\\tag{1.1}\\]\nSolution\n\\[\n\\begin{align}\n\\ell(\\pi) &= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{jk}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln(\\pi_{j+} \\cdot \\pi_{+k}) \\\\\n&= \\sum_{j=1}^R \\sum_{k=1}^C n_{jk} \\ln \\pi_{j+} + n_{jk} \\ln \\pi_{+k} \\\\\n&= \\sum_{j=1}^R n_{j+} \\ln \\pi_{j+} + \\sum_{k=1}^C n_{+k} \\ln \\pi_{+k}\n\\end{align}\n\\tag{1.2}\\]\n2. In a mixed model, optimization is carried out using the marginal likelihood (the likelihood with the random effects integrated out). Define the marginal likelihood for the one-way random effects ANOVA model.\nOne-way random effects ANOVA with group-specific effects \\(\\mu_j \\sim \\mathcal{N}(0, \\sigma^2_{\\mu})\\), and\n\\[\ny_{ij} = \\beta + \\mu_j + \\epsilon_{ij},\n\\]\nwith \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2_\\epsilon)\\), with \\(a\\) groups indexed \\(j\\), and \\(n_j\\) individuals in every group.\nSolution\nSo, the likelihood consists of two components. For the individuals within each group, we have:\n\\[\n\\prod^{n_j}_{i=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\n\\exp \\Bigg(\n-\\frac{(y_{ij} - \\beta - \\mu_j)^2}{2\\sigma^2_{\\epsilon}}\n\\Bigg),\n\\]\nwhereas for the groups themselves, we have\n\\[\n\\prod^{a}_{j=1} \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\mu}}}\n\\exp \\Bigg(\n-\\frac{\\mu_j^2}{2\\sigma^2_{\\mu}}\n\\Bigg).\n\\]\nCombining these components, and integrating out the random effects, we obtain the marginal likelihood\n$$ ^{a}{j=1} ^{n_j}{i=1} \n( - )\n( - ) d_j ~ . $$\n3. Suppose you do a simple linear regression analysis using a \\(t_\\nu\\)-distribution for the residuals (density: \\(f_\\nu(y) = C \\sqrt{\\lambda} \\Big(1 + \\frac{\\lambda(y-\\mu)^2}{\\nu}\\Big)^{-\\frac{\\nu+1}{2}}\\) where \\(\\mu\\) is the mean (for \\(\\nu > 1\\)), \\(\\lambda\\) is a scale parameter and \\(C\\) is a normalizing constraint that does not depend on \\(\\mu\\) or \\(\\lambda\\)). Define the (log-)likelihood for \\(n\\) observations \\((y_i, x_i)\\), such that \\(\\mu_i = \\beta_0 + \\beta_1x_i\\).\nSolution\n\\[\n\\begin{align}\nL(\\beta) &= \\prod_{i=1}^n C\\sqrt{\\lambda} \\Bigg(1 + \\frac{\\lambda (y_i-\\beta_0-\\beta_1x_i)^2}{\\nu}  \\Bigg)^{-\\frac{\\nu+1}{2}}, \\\\\n\\ell(\\beta) &= N \\ln C +  \\frac{N}{2} \\ln \\lambda - \\sum^n_{i=1} \\frac{\\nu + 1}{2} \\ln \\Bigg(1 + \\frac{\\lambda(y_i - \\beta_0 - \\beta_1x_i)^2}{\\nu}\\Bigg)\n\\end{align}\n\\]"
  }
]