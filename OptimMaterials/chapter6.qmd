# Iteration-based Function Optimization

Chapter 2 on motivating problems is the first chapter that actually entails exercises.

## Exercises (6.5 in the notes)

__1. Suppose for every individual in a small pre-clinical study, it has been recorded how many epileptic seizures are observed (outcome $y$) and whether the individual is receiving a standard treatment (covariate $x=0$) or experimental medication (covariate $x=1$). The data are:__

| Subject $i$ | Treatment $x$ | # Seizures $y$ |
|---|---|----|
| 1 | 1 | 12 |
| 2 | 1 | 15 |
| 3 | 1 | 17 |
| 4 | 0 | 8  | 
| 5 | 0 | 11 |
| 6 | 0 | 5  |

__A Poisson regression model is put forward for these data, with linear predictor $\theta_i = \beta_0 + \beta_1 x_i$. Starting from $\boldsymbol{\beta}^{(0)} = (0,0)'$, do the following: Derive the likelihood equations. Can they be solved analytically in this case? Perform the first five steps of the Newton-Raphson algorithm to find the maximum of the likelihood. Put your results in a table with as columns: Iteration number, current point, and log-likelihood value. Do the same for Fisher-scoring.__

_Solution_

The Poisson model yields 
$$
Y \sim \text{Poisson}(\lambda), 
~ \text{with} ~ 
f(y|\theta, \phi) = \frac{e^{-\lambda}\lambda^y}{y!},
$$
and thus the likelihood $L$ and log-likelihood $\ell$ are defined as
$$
\begin{aligned}
L &= \prod^6_{i=1} \frac{e^{-\lambda}\lambda^{y_i}}{y_i!} = \frac{e^{-e^{(\beta_0 + \beta_1 x_i)}} e^{(\beta_0 \beta_1 x_i)y_i}}{y_i!} \\
\ell &= \sum^6_{i=1} y_i \log \lambda - \lambda - \log (y_i!) \\
&= \sum^6_{i=1} y_i(\beta_0 + \beta_1 x_i) - e^{(\beta_0 + \beta_1 x_i)} - \log (y_i!). 
\end{aligned}
$$
Accordingly, the first-order partial derivatives are defined as
$$
\begin{aligned}
\frac{\partial \ell}{\partial \beta_0} &=
\sum^6_{i=1} y_i - e^{(\beta_0 - \beta_1 x_i)}, \\
\frac{\partial \ell}{\partial \beta_1} &=
\sum^6_{i=1} x_i y_i - x_i e^{(\beta_0 - \beta_1 x_i)},
\end{aligned}
$$
and hence the Gradient (i.e., Score equation) can be written as
$$
\nabla \ell(\beta_0, \beta_1) = S(\theta) = \begin{pmatrix}
\sum^6_{i=1} y_i - e^{(\beta_0 - \beta_1 x_i)}, \\
\sum^6_{i=1} x_i (y_i - e^{(\beta_0 - \beta_1 x_i)}).
\end{pmatrix}
$$
Additionally, the second-order partial derivates are defined as
$$
\begin{aligned}
\frac{\partial^2 \ell}{\partial \beta_0^2} &=
\sum^6_{i=1} - e^{(\beta_0 - \beta_1 x_i)}, \\
\frac{\partial^2 \ell}{\partial \beta_1^2} &=
\sum^6_{i=1} - x_i^2 e^{(\beta_0 - \beta_1 x_i)}, \\
\frac{\partial^2 \ell}{\partial \beta_0 \partial \beta_1} &=
\sum^6_{i=1} - x_i e^{(\beta_0 - \beta_1 x_i)}, \\
\end{aligned}
$$
such that the Hessian $\nabla^2 \ell(\beta_0, \beta_1)$ can be written as
$$
\nabla^2 \ell(\beta_0, \beta_1) =
\begin{pmatrix}
\sum^6_{i=1} - e^{(\beta_0 - \beta_1 x_i)} & \\
\sum^6_{i=1} - x_i e^{(\beta_0 - \beta_1 x_i)} &
\sum^6_{i=1} - x_i^2 e^{(\beta_0 - \beta_1 x_i)}
\end{pmatrix}.
$$
Setting the first-order partial derivatives to zero and filling in the data yields
$$
S(\theta) = \begin{pmatrix}
68 - 3e^{(\beta_0 + \beta_1)} - 3e^{(\beta_0)} \\
44 - 3e^{(\beta_0 + \beta_1)}
\end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}.
$$
Hence, we have
$$
\begin{aligned}
44 - 3e^{(\beta_0 + \beta_1)} &= 0 \\
3e^{(\beta_0 + \beta_1)} &= 44,
\end{aligned}
$$
and thus
$$
\begin{aligned}
68 - 3e^{(\beta_0)} &= 44 \\
3e^{(\beta_0)} &= 24 \\
e^{(\beta_0)} &= 8 \\
\beta_0 &= \log 8 \approx `r round(log(8), 4)`.
\end{aligned}
$$
Filling this into the previous equation yields
$$
\begin{aligned}
3e^{(\log 8 + \beta_1)} &= 44 \\
\log 44 - \log 3 - \log 8 &= \beta_1 \approx `r round(log(44) - log(3) - log(8), 4)`.
\end{aligned}
$$

__Newton-Raphson method__

```{r}
NR <- function(formula, data = NULL, start, n.iter) {
  
  X <- model.matrix(formula, data)
  Y <- model.frame(formula, data)[,1]
  
  loglikelihood <- function(X, Y, beta) {
    constant <- sapply(Y, function(y) sum(log(1:y))) |> sum()
    sum(y - X %*% beta - exp(X %*% beta) - constant)
  }
  
  score <- function(X, Y, beta) {
    t(X) %*% (Y - exp(X %*% beta))
  }
  
  hess <- function(X, beta) {
    - t(X) %*% diag(c(exp(X %*% beta))) %*% X
  }
  
  out <- matrix(0, n.iter+1, ncol(X))
  out[1, ] <- b <- start
  
  logL <- numeric(n.iter+1)
  logL[1] <- loglikelihood(X, Y, b)
  
  for (i in (1:n.iter)+1) {
    b <- b - solve(hess(X, b)) %*% score(X, Y, b)
    out[i, ] <- b
    logL[i] <- loglikelihood(X, Y, b)
  }
  data.frame(iter = 0:n.iter,
             out,
             logL = logL)
}

x <- c(1, 1, 1, 0, 0, 0)
y <- c(12, 15, 17, 8, 11, 5)

NR(y ~ x, start = c(0,0), n.iter = 20) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

```

__Fisher scoring__

Note that in this case, the expected Hessian equals 
$$
- x_i' \frac{\partial \mu_i}{\partial \theta_i} \nu_i^{-1} \frac{\partial \mu_i}{\partial \theta_i} x_i. 
$$
Given that
$$
\frac{\partial \mu_i}{\partial \theta_i} = \frac{\partial \mu_i}{\partial \theta_i}
\Bigg(\exp\{\theta_i\}\Bigg) = \exp{\theta_i},
$$
and
$$
\nu_i^{-1} = \frac{1}{\exp\{\theta_i\}},
$$
it follows that
$$
\frac{\partial \mu_i}{\partial \theta_i} \nu_i^{-1} \frac{\partial \mu_i}{\partial \theta_i} = \exp\{\theta_i\} = \exp\{X\beta\}.
$$
Hence, for the expected Hessian, we have
$$
\mathcal{H} = 
E\Bigg(\frac{\partial^2 \ell}{\partial \beta \partial \beta'}\Bigg) =
X' \text{diag}(\exp{X\beta})X,
$$
which is equal to the Hessian matrix $H(\beta)$, and thus Fisher scoring and Newton-Raphson are equivalent in this case.

__2. Assume the function__
$$
f(x_1, x_2) = 8x_1 + 12x_2 + x_1^2 - 2x_2^2.
$$
__Sketch the contour lines of $f(x_1, x_2)$, and find the stationary point of $f(x_1, x_2)$. Does this point correspond to a minimum, a maximum, or something else?__

_Solution_

```{r}
#| message: false
library(ggplot2)
library(dplyr)
library(purrr)

fx1x2 <- function(x1, x2) 8*x1 + 12*x2 + x1^2 - 2*x2^2

expand.grid(x1 = -300:200/10,
            x2 = -200:300/10) |>
  mutate(f = map2_dbl(x1, x2, fx1x2)) |>
  ggplot(aes(x = x1, y = x2, z = f)) +
  stat_contour_filled(bins = 30, show.legend = FALSE) +
  theme_minimal() +
  labs(x = expression(italic(X[1])),
       y = expression(italic(X[2])),
       title = "Contour plot")
```

The first- and second-order partial derivatives of $f(x_1, x_2)$ are given by
$$
\begin{aligned}
f(x1, x2) &= 8x_1 + 12x_2 + x_1^2 - 2x_2^2, \\
\frac{\partial f}{\partial x_1} &= 8 + 2x_1, \\
\frac{\partial f}{\partial x_2} &= 12 - 4x_2, \\
\frac{\partial^2 f}{\partial x_1^2} &= 2, \\
\frac{\partial^2 f}{\partial x_2^2} &= -4, \\
\frac{\partial^2 f}{\partial x_1 \partial x_2} &= 0. \\
\end{aligned}
$$
The stationary point of $f(x_1, x_2)$ is $f(-4, 3)$, which is a saddle point. 

__3. Consider the function__
$$
f(x_1, x_2) = 100(x_2 - x_1^2)^2 + (1 - x_1)^2.
$$
__Show that (1, 1)' is a local minimizer of this function. Also, starting from the point $\boldsymbol{x}^{(0)}=(0,0)'$, perform the first five steps of the steepest descent and the Newton-Raphson algorithm to minimize the function. Put your results in a table with as columns: iteration number, current point, function value and gradient.__

_Solution_

```{r}
fx1x2 <- function(x1, x2) 100*(x2 - x1^2)^2 + (1 - x1)^2

expand.grid(x1 = -100:200/100,
            x2 = -100:200/100) |>
  mutate(f = map2_dbl(x1, x2, fx1x2)) |>
  ggplot(aes(x = x1, y = x2, z = f)) +
  stat_contour_filled(bins = 50, show.legend = FALSE) +
  theme_minimal() +
  labs(x = expression(italic(X[1])),
       y = expression(italic(X[2])),
       title = "Contour plot")
```

Showing that the point $(1,1)'$ is a local minimizer can be done by plugging the $(1,1)'$ into the Gradient, and checking whether the Gradient equals zero,
$$
\begin{aligned}
f(x_1, x_2) &= 100(x_2 - x_1^2)^2 + (1 - x_1)^2, \\
\nabla f(x_1, x_2) &= \begin{pmatrix} 
-400x_1(x_2 - x_1^2) - 2(1 - x_1) \\
200(x_2 - x_1^2) \end{pmatrix}, \\
\nabla f(1,1) &= \begin{pmatrix}
-400(1 - 1) - 2(1 - 1) \\
200(1 - 1) \end{pmatrix} =
\begin{pmatrix} 0 \\ 0 \end{pmatrix},
\end{aligned}
$$
which shows that $(1,1)'$ is a local minimizer. Moreover, the Hessian matrix is defined by
$$
\nabla^2 f(x_1, x_2) = \begin{pmatrix}
1200x_1^2 - 400x_2 + 2 & -400x_1 \\
-400x_1 & 200 \end{pmatrix}
$$

__Steepest-Descent__

```{r}
f <- function(x1, x2) 100*(x2 - x1^2)^2 + (1 - x1)^2

score <- function(x1, x2) {
  c(400*x1^3 - 400*x1*x2 + 2*x1 - 2,
    200*x2 - 200*x1^2)
}
  
hess <- function(x1, x2) {
  matrix(c(1200*x1^2 - 400*x2 + 2, -400*x1, -400*x1, 200), 
         nrow = 2, ncol = 2)
}

SD <- function(start, n.iter, alpha, rho, tol = 1e-16) {
  b <- start
  grad <- matrix(0, n.iter + 1, 2)
  grad[1, ] <- score(b[1], b[2])
  
  out <- matrix(0, n.iter + 1, 2)
  out[1, ] <- b
  
  i <- 1; conv <- FALSE
  
  while (!conv) {
    i <- i+1
    fold <- f(out[i-1, 1], out[i-1, 2])
    gradvec <- score(out[i-1, 1], out[i-1, 2])
    out[i, ] <- out[i-1, ] - alpha * gradvec / sum(gradvec^2)
    grad[i, ] <- gradvec
    
    fnew <- f(out[i,1], out[i,2])
    a <- alpha
    while(fnew > fold) {
      a <- a*rho
      out[i, ] <- out[i-1, ] - a * gradvec / sum(gradvec^2)
      grad[i, ] <- c(score(out[i,1], out[i,2]))
      fnew <- f(out[i,1], out[i,2])
    }
    if (
      i - 1 == n.iter | 
      abs(fnew - fold) < tol
    ) {
      conv <- TRUE
    }
    
  }
  data.frame(iter = 0:(nrow(out)-1), 
             out  = out, 
             grad = grad, 
             fval = f(out[,1], out[,2])) |>
    subset(iter < i)
}

SDout <- SD(c(0,0), 20000, 1, 0.8, 1e-10)

SDout |>
  head(15) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

__Newton-Raphson__

```{r}
NR <- function(start, n.iter, alpha, rho) {
  
  b <- start
  grad <- matrix(0, n.iter + 1, 2)
  grad[1, ] <- score(b[1], b[2])
  
  out <- matrix(0, n.iter + 1, 2)
  out[1, ] <- b
  
  for (i in 1:n.iter + 1) {
    
    fold <- f(out[i-1,1], out[i-1,2])
    b <- out[i - 1, ]
    
    out[i, ] <- b - solve(hess(b[1], b[2])) %*% score(b[1], b[2])
    grad[i, ] <- c(score(b[1], b[2]))

    fnew <- f(out[i,1], out[i,2])
    a <- alpha
    while (fnew>fold){
      a <- a*rho
      out[i,] <- out[i-1,] - a * solve(hess(b[1], b[2])) %*% score(b[1], b[2])
      grad[i, ] <- c(score(out[i,1], out[i,2]))
      fnew <- f(out[i,1], out[i,2])
    }
  }
  data.frame(iter = 0:(nrow(out)-1), 
             out  = out, 
             grad = grad, 
             fval = f(out[,1], out[,2]))
}
NRout <- NR(c(0,0), 15, 1, 0.8) 

NRout |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

expand.grid(x1 = -50:150/100,
            x2 = -50:150/100) |>
  mutate(f = map2_dbl(x1, x2, fx1x2)) |>
  ggplot(aes(x = x1, y = x2, z = f)) +
  stat_contour_filled(bins = 50, show.legend = FALSE) +
  geom_line(data = SDout, 
            mapping = aes(x = out.1, y = out.2, z = NULL), 
            col = "yellow") +
  geom_line(data = NRout, 
            mapping = aes(x = out.1, y = out.2, z = NULL),
            col = "orange") +
  theme_minimal() +
  labs(x = expression(italic(X[1])),
       y = expression(italic(X[2])),
       title = "Contour plot")
```

__4. Suppose for an individual during consecutive nights, it is recorded how loudly he snores (covariate $x$) and whether he wakes up or not (the outcome $Y$). Consider the following hypothetical data are collected: $\boldsymbol{x} = (0,1,2,3,4,5)'$ and $\boldsymbol{y} = (0,1,0,1,1,1)'$. A logistic regression model is put forward for these data such that $\text{logit}(\Pr(y_i = 1 | x_i)) = \text{logit}(\pi(x_i)) = \beta_0 + \beta_1 x_i$. Starting from $\boldsymbol{\beta}^{(0)} = (0,0)'$, perform the first five steps of the Newton-Raphson algorithm to find the maximum of the likelihood. Put your results in a table with as columns: iteration number, current point and loglikelihood value. Do the same for iterative reweighted least squares.__

_Solution_

For logistic regression, the likelihood is defined as
$$
\begin{aligned}
L &= \prod^N_{i=1} \frac{e^{y_i(\beta_0 + \beta_1 x_i)}}{1 + e^{(\beta_0 + \beta_1x_i)}}, \\
\ell &= \sum^N_{i=1} y_i(\beta_0 + \beta_1x_i) - \log(1 + e^{(\beta_0 + \beta_1x_i)}).
\end{aligned}
$$
Additionally, the Gradient is defined by
$$
\nabla \ell(\beta_0, \beta_1) = \begin{pmatrix}
\sum^N_{i=1} y_i - \frac{e^{(\beta_0 + \beta_1 x_i)}}{1 + e^{(\beta_0 + \beta_1 x_i)}} \\
\sum^N_{i=1}x_i (y_i - \frac{e^{(\beta_0 + \beta_1 x_i)}}{1 + e^{(\beta_0 + \beta_1 x_i)}})
\end{pmatrix},
$$
while the Hessian is defined as
$$
\nabla^2 \ell(\beta_0, \beta_1) = \begin{pmatrix}
- \sum^N_{i=1} \frac{e^{(\beta_0 + \beta_1 x_i)}}{(1 + e^{(\beta_0 + \beta_1 x_i)})^2} & 
- \sum^N_{i=1}x_i \frac{e^{(\beta_0 + \beta_1 x_i)}}{(1 + e^{(\beta_0 + \beta_1 x_i)})^2} \\
- \sum^N_{i=1}x_i \frac{e^{(\beta_0 + \beta_1 x_i)}}{(1 + e^{(\beta_0 + \beta_1 x_i)})^2} &
- \sum^N_{i=1} x_i^2\frac{e^{(\beta_0 + \beta_1 x_i)}}{(1 + e^{(\beta_0 + \beta_1 x_i)})^2}
\end{pmatrix}.
$$

```{r}
loglikelihood <- function(X, Y, beta) {
  sum(Y * (X%*%beta) - log(1 + exp(X %*% beta)))
}
score <- function(X, Y, beta) {
  t(X) %*% (Y - 1/(1 + exp(-X%*%beta)))
}
hess <- function(X, Y, beta) {
  - t(X) %*% diag(c(exp(X%*%beta)/(1 + exp(X%*%beta))^2)) %*% X
}
```

__Newton-Raphson implementation__

```{r}
NRlogistic <- function(formula, data = NULL, start, n.iter) {
  X <- model.matrix(formula, data)
  Y <- model.frame(formula, data)[, 1]
  
  out <- matrix(0, n.iter+1, ncol(X))
  out[1, ] <- b <- start
  
  logL <- numeric(n.iter+1)
  logL <- loglikelihood(X, Y, b)
  
  for (i in 1:n.iter + 1) {
    b <- b - solve(hess(X, Y, b)) %*% score(X, Y, b)
    out[i, ] <- b
    logL[i] <- loglikelihood(X, Y, b)
  }
  
  data.frame(iter = 0:n.iter,
             b0 = out[,1],
             b1 = out[,2],
             logL = logL)
}

x <- c(0,1,2,3,4,5)
y <- c(0,1,0,1,1,1)

NRlogistic(y ~ x, start = c(0,0), n.iter = 5) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

glm(y ~ x, family = binomial) |> summary()
```
Convergence is reached after five iterations!

__Iterative re-weighted least squares implementation__

```{r}
IRLS <- function(formula, data = NULL, start, n.iter) {
  
  X <- model.matrix(formula, data)
  Y <- model.frame(formula, data)[,1]
  out <- matrix(0, n.iter+1, ncol(X))
  out[1, ] <- b <- start
  
  logL <- numeric(n.iter+1)
  logL[1] <- loglikelihood(X, Y, b)
  
  for (i in 1:n.iter+1) {
    e <- exp(X%*%b) / (1 + exp(X%*%b))
    W <- diag(c(e / (1+exp(X %*% b))))
    Z <- X %*% b + (y - e) * (1 / (e*(1-e)))
    b <- solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% Z
    out[i, ] <- b
    logL[i] <- loglikelihood(X, Y, b)
  }
  data.frame(iter = 0:n.iter,
             b0 = out[,1],
             b1 = out[,2],
             logL = logL)
}
IRLS(y ~ x, start = c(0,0), n.iter = 5) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

And again, convergence is reached after five iterations!

__5. Consider the function__
$$
f(x) = \frac{e^x}{(1 + e^x)^2}.
$$
__Using an iterative procedure of your liking, find the optimum of the function, and check whether it is a minimum or a maximum.__

_Solution_

First, calculate we calculate the derivatives. 
$$
\begin{aligned}
f(x) &= \frac{e^x}{(1 + e^x)^2}, \\
f'(x) &= \frac{e^x - e^{2x}}{(1+e^x)^3}, \\
f''(x) &= \frac{e^x - 4e^{2x} + e^{3x}}{(1+e^x)^4}.
\end{aligned}
$$
We can first find the optimum analytically. Let's first take the log of the function, which makes it easier to work with:
$$
\log f(x) = x - 2 \log (1+e^x).
$$
Subsequently, we take the derivative of $\log f(x)$ and set it equal to zero to find the optimum.
$$
\begin{aligned}
\frac{\partial f}{\partial x} = 1 - \frac{2e^x}{1+e^x} &= 0, \\
\implies \frac{2e^x}{1+e^x} &= 1, \\
1+e^x &= 2e^x, \\
e^x &= 1, \\
x &= 0.
\end{aligned}
$$
So we know the solution must be $x=0$. Doing the same steps using the Newton-Raphson algorithm yields

```{r} 
fx <- function(x) -exp(x) / (1 + exp(x))^2
f1x <- function(x) -(exp(x) - exp(2*x)) / (1 + exp(x))^3
f2x <- function(x) -(exp(x) - 4*exp(2*x) + exp(3*x)) / (1 + exp(x))^4

NR <- function(start = 0.5, n.iter = 20, alpha = 1, rho = 0.8) {
  out <- matrix(0, n.iter+1, 4)
  out[1, ] <- c(start, fx(start), f1x(start), f2x(start))
  
  colnames(out) <- c("x", "fx", "f1x", "f2x")
  
  for (i in 1:n.iter + 1) {
    a <- alpha
    new <- out[i-1, 1] - a * out[i-1, 3] / out[i-1, 4]
    out[i, ] <- c(new, fx(new), f1x(new), f2x(new))
    while(out[i, 2] > out[i-1, 2]) {
      a <- a*rho
      new <- out[i-1, 1] - a * out[i-1, 3] / out[i-1, 4]
      out[i, ] <- c(new, fx(new), f1x(new), f2x(new))
    }
  }
  out
}

NR(0.5, 5) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))
```

__6. Continuation of exercise 6 from chapter 3: implement maximum likelihood estimation for this logistic regression.__

_Solution_

```{r}
x <- c(0.5, 1, 1.5, 2, 2.5)
y <- c(0,0,1,0,1)

NRlogistic(y ~ x, start = c(0,0), n.iter = 5) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

IRLS(y ~ x, start = c(0,0), n.iter = 5) |>
  knitr::kable() |>
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

glm(y ~ x, family = binomial) |> summary()
```

